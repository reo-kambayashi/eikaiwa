"""
FastAPI backend for the English Communication App.

This backend server provides AI-powered English conversation practice
for Japanese learners using Google Gemini AI and Google Cloud TTS.

Key features:
- AI conversation responses using Gemini
- Text-to-speech synthesis for pronunciation practice
- Grammar checking and feedback
- Voice input and output customization
"""

import asyncio
import base64
import os
from concurrent.futures import ThreadPoolExecutor

import google.generativeai as genai
# Import configuration and setup from config.py
from config import (CACHE_TTL, GEMINI_API_KEY, GOOGLE_APPLICATION_CREDENTIALS,
                    executor, model, response_cache, tts_model)
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
# Import all models from the separate models.py file
from models import (CombinedResponse, InstantTranslationCheckRequest,
                    InstantTranslationCheckResponse, InstantTranslationProblem,
                    JapaneseConsultationRequest, ListeningAnswerRequest,
                    ListeningAnswerResponse, ListeningProblem,
                    ListeningTranslateRequest, ListeningTranslateResponse,
                    Request)
from models import Response as ResponseModel
from models import TTSRequest
from pydantic import BaseModel
# Import AI service functions
from services.ai_service import (create_conversation_prompt,
                                 create_eiken_problem_generation_prompt,
                                 create_japanese_consultation_prompt,
                                 create_translation_check_prompt,
                                 create_welcome_prompt)
# Import listening service  
from services.listening_service import (fetch_trivia_question,
                                        get_trivia_categories)
# Import translation service data
from services.translation_service import TRANSLATION_PROBLEMS
# Import TTS service
from services.tts_service import synthesize_speech

# Create FastAPI application instance
app = FastAPI()

# Add CORS middleware to allow frontend connections from React development server
# This is necessary for the frontend (localhost:3000) to communicate with backend (localhost:8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods (GET, POST, etc.)
    allow_headers=["*"],  # Allow all headers
)

# API Endpoints
# These endpoints handle communication between the frontend and backend

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
import time


@app.get("/")
async def root():
    """
    Health check endpoint - confirms the API server is running.

    This is useful for monitoring and debugging server status.
    Returns a simple JSON message when the server is operational.
    """
    return {"message": "English Communication App API is running"}


@app.get("/health")
async def health_check():
    """
    Health check endpoint for Docker container monitoring.

    This endpoint is used by Docker's HEALTHCHECK instruction
    to verify that the application is running properly.

    Returns:
        dict: Simple status message indicating the service is healthy
    """
    return {"status": "healthy", "service": "eikaiwa-backend"}


@app.get("/api/status")
async def api_status():
    """
    Configuration status endpoint - checks if required services are available.

    Returns the status of:
    - Gemini AI API (for conversation generation)
    - Google Cloud credentials (for authentication)
    - TTS service (for speech synthesis)

    This helps users troubleshoot configuration issues.
    """
    return {
        "gemini_configured": bool(GEMINI_API_KEY and model),
        "google_credentials_configured": bool(
            GOOGLE_APPLICATION_CREDENTIALS
            and os.path.exists(GOOGLE_APPLICATION_CREDENTIALS)
        ),
        "gemini_tts_configured": bool(GEMINI_API_KEY and tts_model),
        "tts_configured": bool(tts_model),
    }


@app.post("/api/tts")
async def text_to_speech(request: TTSRequest):
    """Convert text to speech using Gemini TTS."""

    if not tts_model:
        raise HTTPException(
            status_code=503, detail="TTS service not available"
        )

    try:
        # TTSã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        import time

        tts_cache_key = f"tts_{hash(request.text)}_{request.voice_name}_{request.speaking_rate}"
        if tts_cache_key in response_cache:
            cached_data, timestamp = response_cache[tts_cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"âœ… TTS Cache hit for: {request.text[:30]}...")
                return cached_data

        # Gemini 2.5 Flash Preview TTS with dictionary-based config
        content = request.text

        # Configure generation with dictionary format
        generation_config = {
            "response_modalities": ["AUDIO"],
            "speech_config": {
                "voice_config": {
                    "prebuilt_voice_config": {"voice_name": request.voice_name}
                }
            },
        }

        # Generate audio using Gemini TTS model (éåŒæœŸå®Ÿè¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor,
            lambda: tts_model.generate_content(
                contents=content, generation_config=generation_config
            ),
        )

        # Extract audio data from response
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                for part in candidate.content.parts:
                    if hasattr(part, "inline_data") and part.inline_data:
                        # Found audio data - extract properly
                        audio_data = part.inline_data.data
                        mime_type = part.inline_data.mime_type or "audio/wav"

                        print(
                            f"ğŸµ Audio data found: type={type(audio_data)}, mime_type={mime_type}"
                        )

                        # Handle different data types from Gemini
                        if isinstance(audio_data, bytes):
                            # If bytes, encode to base64
                            audio_base64 = base64.b64encode(audio_data).decode(
                                "utf-8"
                            )
                            print(
                                f"ğŸ”„ Encoded bytes to base64: {len(audio_data)} bytes -> {len(audio_base64)} chars"
                            )
                        elif isinstance(audio_data, str):
                            # If already string, assume it's base64
                            audio_base64 = audio_data
                            print(
                                f"ğŸ”„ Using string as base64: {len(audio_base64)} chars"
                            )
                        else:
                            print(
                                f"âŒ Unexpected audio data type: {type(audio_data)}"
                            )
                            raise ValueError(
                                f"Unexpected audio data type: {type(audio_data)}"
                            )

                        # Validate base64 data
                        try:
                            # Test decode to verify it's valid base64
                            decoded_test = base64.b64decode(audio_base64)
                            print(
                                f"âœ… Base64 validation successful: {len(decoded_test)} bytes decoded"
                            )
                        except Exception as b64_error:
                            print(f"âŒ Base64 validation failed: {b64_error}")
                            raise ValueError(
                                f"Invalid base64 audio data: {b64_error}"
                            )

                        result = {
                            "audio_data": audio_base64,
                            "content_type": mime_type,
                            "original_size": (
                                len(audio_data)
                                if isinstance(audio_data, (bytes, str))
                                else 0
                            ),
                        }
                        # TTSãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                        response_cache[tts_cache_key] = (result, time.time())
                        return result

        # If no audio data found, fallback to browser TTS
        print("No audio data found in Gemini TTS response")
        fallback_result = {
            "audio_data": "",
            "content_type": "text/plain",
            "fallback_text": request.text,
            "use_browser_tts": True,
        }
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        response_cache[tts_cache_key] = (fallback_result, time.time())
        return fallback_result

    except HTTPException:
        # Propagate HTTP errors such as 503 without modification
        raise
    except Exception as e:
        print(f"Gemini TTS Error: {str(e)}")
        # Fallback to browser TTS
        error_result = {
            "audio_data": "",
            "content_type": "text/plain",
            "fallback_text": request.text,
            "use_browser_tts": True,
            "error": str(e),
        }
        # ã‚¨ãƒ©ãƒ¼çµæœã¯çŸ­æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ30ç§’ï¼‰
        response_cache[tts_cache_key] = (
            error_result,
            time.time() - CACHE_TTL + 30,
        )
        return error_result


@app.get("/api/welcome", response_model=ResponseModel)
async def get_welcome_message():
    """Generate a personalized welcome message."""

    print("ğŸ”” Welcome request received")

    try:
        if not model:
            return ResponseModel(
                reply="Hello! Welcome to English Communication App! Please set up your API key to get started."
            )

        welcome_prompt = create_welcome_prompt()
        # AIç”Ÿæˆã‚’éåŒæœŸå®Ÿè¡Œ
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(welcome_prompt)
        )

        if response.text:
            return ResponseModel(reply=response.text)
        else:
            return ResponseModel(
                reply="Hello! Welcome to English Communication App! Let's start practicing English together!"
            )

    except Exception as e:
        print(f"Error generating welcome message: {str(e)}")
        return ResponseModel(
            reply="Hello! Welcome to English Communication App! I'm here to help you practice English. How are you today?"
        )


@app.post("/api/respond", response_model=ResponseModel)
async def respond(req: Request):
    """Generate a response using Gemini API for English conversation practice."""

    print(f"ğŸ”” Response request received: text='{req.text[:50]}...'")

    try:
        if not model:
            # Fallback response if Gemini API is not configured
            return ResponseModel(
                reply="API key not configured. Please set GEMINI_API_KEY environment variable."
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = (
            f"response_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )
        import time

        if cache_key in response_cache:
            cached_data, timestamp = response_cache[cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"âœ… Cache hit for response: {req.text[:30]}...")
                return ResponseModel(reply=cached_data)

        # Create conversation prompt
        prompt = create_conversation_prompt(req.text, req.conversation_history)

        # Generate response using Gemini (éåŒæœŸå®Ÿè¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        if response.text:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            response_cache[cache_key] = (response.text, time.time())
            return ResponseModel(reply=response.text)
        else:
            return ResponseModel(
                reply="Sorry, I couldn't generate a response. Please try again."
            )

    except Exception as e:
        # Log the error in production, but don't expose internal details
        print(f"Error generating response: {str(e)}")
        return ResponseModel(
            reply="Sorry, there was an error processing your request. Please try again."
        )


@app.post("/api/japanese-consultation", response_model=ResponseModel)
async def japanese_consultation(req: JapaneseConsultationRequest):
    """Generate Japanese consultation response for English expression and grammar questions."""

    print(f"ğŸ”” Japanese consultation request: text='{req.text[:50]}...'")

    try:
        if not model:
            # Fallback response if Gemini API is not configured
            return ResponseModel(
                reply="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = (
            f"consultation_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )
        import time

        if cache_key in response_cache:
            cached_data, timestamp = response_cache[cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"âœ… Cache hit for consultation: {req.text[:30]}...")
                return ResponseModel(reply=cached_data)

        # Create Japanese consultation prompt
        prompt = create_japanese_consultation_prompt(
            req.text, "general", req.conversation_history
        )

        # Generate response using Gemini (éåŒæœŸå®Ÿè¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        if response.text:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            response_cache[cache_key] = (response.text, time.time())
            return ResponseModel(reply=response.text)
        else:
            return ResponseModel(
                reply="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            )

    except Exception as e:
        # Log the error in production, but don't expose internal details
        print(f"Error generating Japanese consultation response: {str(e)}")
        return ResponseModel(
            reply="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        )


@app.post("/api/respond-with-audio", response_model=CombinedResponse)
async def respond_with_audio(
    req: Request, voice_name: str = "Kore", speaking_rate: float = 1.0
):
    """
    Generate both text response and audio simultaneously for optimal performance.

    This endpoint combines conversation generation and TTS processing
    to reduce total response time for single-user scenarios.
    """

    print(
        f"ğŸ”” Combined response request: text='{req.text[:50]}...', voice={voice_name}"
    )
    start_time = time.time()

    try:
        if not model:
            return CombinedResponse(
                reply="API key not configured. Please set GEMINI_API_KEY environment variable.",
                use_browser_tts=True,
                fallback_text="API key not configured. Please set GEMINI_API_KEY environment variable.",
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        import time

        cache_key = (
            f"response_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        prompt = create_conversation_prompt(req.text, req.conversation_history)
        loop = asyncio.get_event_loop()

        # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã‚’éåŒæœŸå®Ÿè¡Œ
        response_future = loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¾…ã¤
        ai_response = await response_future

        if not ai_response.text:
            return CombinedResponse(
                reply="Sorry, I couldn't generate a response. Please try again.",
                use_browser_tts=True,
                fallback_text="Sorry, I couldn't generate a response. Please try again.",
            )

        reply_text = ai_response.text

        # TTSç”Ÿæˆã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆAIãƒ¬ã‚¹ãƒãƒ³ã‚¹å¾Œï¼‰
        if tts_model:
            tts_cache_key = (
                f"tts_{hash(reply_text)}_{voice_name}_{speaking_rate}"
            )

            # TTSã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if tts_cache_key in response_cache:
                cached_tts, timestamp = response_cache[tts_cache_key]
                if time.time() - timestamp < CACHE_TTL:
                    print(f"âœ… TTS Cache hit for combined response")
                    processing_time = time.time() - start_time
                    return CombinedResponse(
                        reply=reply_text,
                        audio_data=cached_tts.get("audio_data", ""),
                        content_type=cached_tts.get(
                            "content_type", "text/plain"
                        ),
                        use_browser_tts=cached_tts.get(
                            "use_browser_tts", False
                        ),
                        fallback_text=cached_tts.get(
                            "fallback_text", reply_text
                        ),
                        processing_time=processing_time,
                    )

            # TTSç”Ÿæˆè¨­å®š
            generation_config = {
                "response_modalities": ["AUDIO"],
                "speech_config": {
                    "voice_config": {
                        "prebuilt_voice_config": {"voice_name": voice_name}
                    }
                },
            }

            try:
                # TTSç”Ÿæˆã‚’éåŒæœŸå®Ÿè¡Œ
                tts_response = await loop.run_in_executor(
                    executor,
                    lambda: tts_model.generate_content(
                        contents=reply_text,
                        generation_config=generation_config,
                    ),
                )

                # TTSã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                audio_data = ""
                content_type = "text/plain"
                use_browser_tts = True

                if (
                    tts_response.candidates
                    and len(tts_response.candidates) > 0
                ):
                    candidate = tts_response.candidates[0]
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if (
                                hasattr(part, "inline_data")
                                and part.inline_data
                            ):
                                raw_audio = part.inline_data.data
                                content_type = (
                                    part.inline_data.mime_type or "audio/wav"
                                )

                                if isinstance(raw_audio, bytes):
                                    audio_data = base64.b64encode(
                                        raw_audio
                                    ).decode("utf-8")
                                elif isinstance(raw_audio, str):
                                    audio_data = raw_audio

                                if audio_data:
                                    use_browser_tts = False
                                    break

                # TTSçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                tts_result = {
                    "audio_data": audio_data,
                    "content_type": content_type,
                    "use_browser_tts": use_browser_tts,
                    "fallback_text": reply_text if use_browser_tts else "",
                }
                response_cache[tts_cache_key] = (tts_result, time.time())

                processing_time = time.time() - start_time
                print(
                    f"âš™ï¸ Combined processing completed in {processing_time:.2f}s"
                )

                return CombinedResponse(
                    reply=reply_text,
                    audio_data=audio_data,
                    content_type=content_type,
                    use_browser_tts=use_browser_tts,
                    fallback_text=reply_text if use_browser_tts else "",
                    processing_time=processing_time,
                )

            except Exception as tts_error:
                print(f"TTS Error in combined response: {str(tts_error)}")
                # TTSã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ–ãƒ©ã‚¦ã‚¶TTSã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                processing_time = time.time() - start_time
                return CombinedResponse(
                    reply=reply_text,
                    use_browser_tts=True,
                    fallback_text=reply_text,
                    processing_time=processing_time,
                )

        # TTSãƒ¢ãƒ‡ãƒ«ãŒç„¡ã„å ´åˆ
        processing_time = time.time() - start_time
        return CombinedResponse(
            reply=reply_text,
            use_browser_tts=True,
            fallback_text=reply_text,
            processing_time=processing_time,
        )

    except Exception as e:
        print(f"Error in combined response: {str(e)}")
        processing_time = time.time() - start_time
        return CombinedResponse(
            reply="Sorry, there was an error processing your request. Please try again.",
            use_browser_tts=True,
            fallback_text="Sorry, there was an error processing your request. Please try again.",
            processing_time=processing_time,
        )


# ============================================================================
# ç¬é–“è‹±ä½œæ–‡ãƒ¢ãƒ¼ãƒ‰ç”¨ã®API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================

# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œãƒ¢ãƒ¼ãƒ‰ç”¨ã®API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================

@app.get(
    "/api/instant-translation/problem",
    response_model=InstantTranslationProblem,
)
async def get_instant_translation_problem(
    difficulty: str = "all",
    category: str = "all",
    eiken_level: str = "",
    long_text_mode: bool = False,
):
    """
    ç¬é–“è‹±ä½œæ–‡ã®å•é¡Œã‚’å–å¾—ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    é›£æ˜“åº¦ã€ã‚«ãƒ†ã‚´ãƒªã€è‹±æ¤œãƒ¬ãƒ™ãƒ«ã«åŸºã¥ã„ã¦é©åˆ‡ãªå•é¡Œã‚’è¿”ã—ã¾ã™ã€‚
    è‹±æ¤œãƒ¬ãƒ™ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€AIã‚’ä½¿ã£ã¦å‹•çš„ã«å•é¡Œã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        difficulty: å•é¡Œã®é›£æ˜“åº¦ (all, basic, intermediate, advanced)
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (all, daily_life, work, travel, etc.)
        eiken_level: è‹±æ¤œãƒ¬ãƒ™ãƒ« (5, 4, 3, pre-2, 2, pre-1, 1)
    """

    print(
        f"ğŸ”” Instant translation problem request: difficulty={difficulty}, category={category}, eiken_level={eiken_level}, long_text_mode={long_text_mode}"
    )

    try:
        import json
        import random

        # è‹±æ¤œãƒ¬ãƒ™ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¦ã€AIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯AIç”Ÿæˆã‚’è©¦è¡Œ
        if eiken_level and eiken_level.strip() and model:
            print(f"ğŸ¤– Generating AI problem for Eiken level {eiken_level}")

            try:
                # ã‚«ãƒ†ã‚´ãƒªã®ãƒãƒƒãƒ”ãƒ³ã‚°
                category_for_ai = category if category != "all" else "general"

                # AIå•é¡Œç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                ai_prompt = create_eiken_problem_generation_prompt(
                    eiken_level, category_for_ai, long_text_mode
                )

                # AIã«å•é¡Œç”Ÿæˆã‚’ä¾é ¼ï¼ˆéåŒæœŸå®Ÿè¡Œï¼‰
                loop = asyncio.get_event_loop()
                ai_response = await loop.run_in_executor(
                    executor, lambda: model.generate_content(ai_prompt)
                )

                if ai_response.text:
                    # AIã®å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦JSONã‚’æŠ½å‡º
                    ai_text = ai_response.text.strip()

                    # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
                    json_start = ai_text.find("{")
                    json_end = ai_text.rfind("}") + 1

                    if json_start != -1 and json_end > json_start:
                        json_text = ai_text[json_start:json_end]

                        try:
                            ai_problem = json.loads(json_text)

                            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                            if all(
                                key in ai_problem
                                for key in ["japanese", "english"]
                            ):
                                print(f"âœ… AI generated problem successfully")

                                # é›£æ˜“åº¦ã¨ã‚«ãƒ†ã‚´ãƒªã‚’èª¿æ•´
                                eiken_to_difficulty = {
                                    "5": "easy",
                                    "4": "easy",
                                    "3": "medium",
                                    "pre-2": "medium",
                                    "2": "medium",
                                    "pre-1": "hard",
                                    "1": "hard",
                                }

                                return InstantTranslationProblem(
                                    japanese=ai_problem["japanese"],
                                    english=ai_problem["english"],
                                    difficulty=ai_problem.get(
                                        "difficulty",
                                        eiken_to_difficulty.get(
                                            eiken_level, "medium"
                                        ),
                                    ),
                                    category=ai_problem.get(
                                        "category", category_for_ai
                                    ),
                                )
                            else:
                                print(
                                    f"âš ï¸ AI response missing required fields, falling back to static problems"
                                )
                        except json.JSONDecodeError as e:
                            print(
                                f"âš ï¸ Failed to parse AI JSON response: {e}, falling back to static problems"
                            )
                    else:
                        print(
                            f"âš ï¸ No valid JSON found in AI response, falling back to static problems"
                        )
                else:
                    print(
                        f"âš ï¸ Empty AI response, falling back to static problems"
                    )

            except Exception as e:
                print(
                    f"âš ï¸ AI problem generation failed: {e}, falling back to static problems"
                )

        # é™çš„å•é¡Œãƒªã‚¹ãƒˆã‹ã‚‰ã®é¸æŠï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print(f"ğŸ“š Using static problem list")

        # è‹±æ¤œãƒ¬ãƒ™ãƒ«ã‚’é›£æ˜“åº¦ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        eiken_to_difficulty = {
            "5": "easy",
            "4": "easy",
            "3": "medium",
            "pre-2": "medium",
            "2": "medium",
            "pre-1": "hard",
            "1": "hard",
        }

        # é›£æ˜“åº¦ã®æ±ºå®š - è‹±æ¤œãƒ¬ãƒ™ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆ
        if eiken_level and eiken_level in eiken_to_difficulty:
            target_difficulty = eiken_to_difficulty[eiken_level]
        elif difficulty != "all":
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®é›£æ˜“åº¦ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å½¢å¼ã«å¤‰æ›
            difficulty_mapping = {
                "basic": "easy",
                "intermediate": "medium",
                "advanced": "hard",
            }
            target_difficulty = difficulty_mapping.get(difficulty, "medium")
        else:
            target_difficulty = "all"

        # å•é¡Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_problems = []

        # é›£æ˜“åº¦ãƒ•ã‚£ãƒ«ã‚¿
        if target_difficulty == "all":
            filtered_problems = TRANSLATION_PROBLEMS.copy()
        else:
            filtered_problems = [
                p
                for p in TRANSLATION_PROBLEMS
                if p["difficulty"] == target_difficulty
            ]

        # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
        if category != "all":
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ã‚«ãƒ†ã‚´ãƒªåã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å½¢å¼ã«å¤‰æ›
            category_mapping = {
                "daily_life": ["daily_life", "daily_routine", "preferences"],
                "work": ["business", "work"],
                "travel": ["travel", "transportation"],
                "education": ["learning", "education"],
                "technology": ["technology"],
                "health": ["health"],
                "culture": ["general"],  # ä»Šå¾Œè¿½åŠ äºˆå®š
                "environment": ["general"],  # ä»Šå¾Œè¿½åŠ äºˆå®š
            }

            target_categories = category_mapping.get(category, [category])
            filtered_problems = [
                p
                for p in filtered_problems
                if p["category"] in target_categories
            ]
        # åˆ©ç”¨å¯èƒ½ãªå•é¡ŒãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not filtered_problems:
            print(f"No problems found for filters, using fallback")
            filtered_problems = TRANSLATION_PROBLEMS.copy()

        # ãƒ©ãƒ³ãƒ€ãƒ ã«å•é¡Œã‚’é¸æŠ
        problem = random.choice(filtered_problems)

        return InstantTranslationProblem(
            japanese=problem["japanese"],
            english=problem["english"],
            difficulty=problem["difficulty"],
            category=problem["category"],
        )

    except Exception as e:
        print(f"Error generating instant translation problem: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å•é¡Œ
        fallback_problem = {
            "japanese": "ç§ã¯æ¯æ—¥è‹±èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚",
            "english": "I study English every day.",
            "difficulty": "easy",
            "category": "daily_life",
        }

        return InstantTranslationProblem(
            japanese=fallback_problem["japanese"],
            english=fallback_problem["english"],
            difficulty=fallback_problem["difficulty"],
            category=fallback_problem["category"],
        )


# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œå–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================


@app.get("/api/listening/problem", response_model=ListeningProblem)
async def get_listening_problem(
    category: str = "any",
    difficulty: str = "medium",
    _t: str = None,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°ç”¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
):
    """
    Trivia APIã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã‚’å–å¾—ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Args:
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (any, sports, science, history, etc.)
        difficulty: é›£æ˜“åº¦ (easy, medium, hard)
        _t: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°ç”¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆå†…éƒ¨ã§ã¯ä½¿ç”¨ã—ãªã„ï¼‰

    Returns:
        ListeningProblem: å•é¡Œæ–‡ã€é¸æŠè‚¢ã€æ­£è§£ã€é›£æ˜“åº¦ã€ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€
    """
    try:
        import httpx

        # Open Trivia Database APIã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        base_url = "https://opentdb.com/api.php"
        params = {
            "amount": 1,  # 1å•å–å¾—
            "type": "multiple",  # å¤šè‚¢é¸æŠå•é¡Œ
            "difficulty": difficulty,
            "encode": "url3986",  # RFC 3986 URL ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        }

        # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆOpen Trivia DBã®ã‚«ãƒ†ã‚´ãƒªIDï¼‰
        category_mapping = {
            "any": None,
            "general": 9,
            "books": 10,
            "film": 11,
            "music": 12,
            "television": 14,
            "science": 17,
            "computers": 18,
            "math": 19,
            "mythology": 20,
            "sports": 21,
            "geography": 22,
            "history": 23,
            "politics": 24,
            "art": 25,
            "celebrities": 26,
            "animals": 27,
            "vehicles": 28,
        }

        # ã‚«ãƒ†ã‚´ãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
        if category != "any" and category in category_mapping:
            params["category"] = category_mapping[category]

        # Trivia APIã‹ã‚‰å•é¡Œã‚’å–å¾—ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™è€ƒæ…®ï¼‰
        import asyncio
        import time

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ5ç§’é–“éš”ï¼‰
        current_time = time.time()
        if hasattr(get_listening_problem, "_last_request_time"):
            time_since_last = (
                current_time - get_listening_problem._last_request_time
            )
            if time_since_last < 5.0:
                wait_time = 5.0 - time_since_last
                print(f"â³ Rate limit: waiting {wait_time:.1f} seconds")
                await asyncio.sleep(wait_time)

        get_listening_problem._last_request_time = time.time()

        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        response_code = data.get("response_code", -1)

        if response_code == 1:
            raise Exception(
                "API Error: Not enough questions for the specified criteria"
            )
        elif response_code == 2:
            raise Exception("API Error: Invalid parameters")
        elif response_code == 3:
            raise Exception("API Error: Token not found")
        elif response_code == 4:
            raise Exception("API Error: Token exhausted")
        elif response_code == 5:
            raise Exception("API Error: Rate limit exceeded")
        elif response_code != 0:
            raise Exception(
                f"API Error: Unknown response code {response_code}"
            )

        if not data.get("results"):
            raise Exception("No questions returned from API")

        # å•é¡Œãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        question_data = data["results"][0]

        # URL ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        import urllib.parse

        question = urllib.parse.unquote(question_data["question"])
        correct_answer = urllib.parse.unquote(question_data["correct_answer"])
        incorrect_answers = [
            urllib.parse.unquote(ans)
            for ans in question_data["incorrect_answers"]
        ]

        # é¸æŠè‚¢ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        import random

        choices = [correct_answer] + incorrect_answers
        random.shuffle(choices)

        return ListeningProblem(
            question=question,
            choices=choices,
            correct_answer=correct_answer,
            difficulty=question_data["difficulty"],
            category=question_data["category"],
            explanation="",  # Trivia APIã«ã¯è§£èª¬ãŒãªã„ãŸã‚ç©ºæ–‡å­—
        )

    except Exception as e:
        print(f"Error fetching listening problem: {str(e)}")

        # å……å®Ÿã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å•é¡Œã‚»ãƒƒãƒˆ
        fallback_problems = [
            {
                "question": "What is the capital of Japan?",
                "choices": ["Tokyo", "Osaka", "Kyoto", "Hiroshima"],
                "correct_answer": "Tokyo",
                "difficulty": "easy",
                "category": "Geography",
            },
            {
                "question": "Which planet is known as the Red Planet?",
                "choices": ["Venus", "Mars", "Jupiter", "Saturn"],
                "correct_answer": "Mars",
                "difficulty": "easy",
                "category": "Science",
            },
            {
                "question": "How many continents are there on Earth?",
                "choices": ["5", "6", "7", "8"],
                "correct_answer": "7",
                "difficulty": "medium",
                "category": "Geography",
            },
            {
                "question": "What is the largest mammal in the world?",
                "choices": [
                    "Elephant",
                    "Blue Whale",
                    "Giraffe",
                    "Hippopotamus",
                ],
                "correct_answer": "Blue Whale",
                "difficulty": "medium",
                "category": "Science",
            },
        ]

        # é›£æ˜“åº¦ã«å¿œã˜ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å•é¡Œã‚’é¸æŠ
        suitable_problems = [
            p for p in fallback_problems if p["difficulty"] == difficulty
        ]
        if not suitable_problems:
            suitable_problems = (
                fallback_problems  # é©åˆ‡ãªé›£æ˜“åº¦ãŒãªã„å ´åˆã¯å…¨ã¦
            )

        import random

        selected_problem = random.choice(suitable_problems)

        return ListeningProblem(
            question=selected_problem["question"],
            choices=selected_problem["choices"],
            correct_answer=selected_problem["correct_answer"],
            difficulty=selected_problem["difficulty"],
            category=selected_problem["category"],
            explanation="This is a fallback question due to external API issues.",
        )


# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œå›ç­”ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================


@app.post("/api/listening/check", response_model=ListeningAnswerResponse)
async def check_listening_answer(req: ListeningAnswerRequest):
    """
    ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Args:
        req: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ãƒ‡ãƒ¼ã‚¿

    Returns:
        ListeningAnswerResponse: æ­£è§£åˆ¤å®šã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€è§£èª¬
    """
    try:
        # æ­£è§£åˆ¤å®šï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ï¼‰
        is_correct = (
            req.user_answer.strip().lower()
            == req.correct_answer.strip().lower()
        )

        # AIã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆ
        if model:
            prompt = f"""
ã‚ãªãŸã¯è‹±èªå­¦ç¿’è€…å‘ã‘ã®ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œãƒãƒ¥ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å›ç­”ã«ã¤ã„ã¦ã€åŠ±ã¾ã—ã¨ã¨ã‚‚ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {req.question}
é¸æŠè‚¢: {', '.join(req.choices)}
æ­£è§£: {req.correct_answer}
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”: {req.user_answer}
æ­£è§£åˆ¤å®š: {'æ­£è§£' if is_correct else 'ä¸æ­£è§£'}

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ä»¥ä¸‹ã®è¦ç´ ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
1. æ­£è§£ãƒ»ä¸æ­£è§£ã®åˆ¤å®š
2. æ­£è§£ã®ç†ç”±ã‚„èƒŒæ™¯çŸ¥è­˜
3. å­¦ç¿’è€…ã¸ã®åŠ±ã¾ã—ã®è¨€è‘‰
4. æ—¥æœ¬èªã§100æ–‡å­—ä»¥å†…

å›ç­”å½¢å¼ï¼šJSON
{{
    "feedback": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ–‡",
    "explanation": "è§£èª¬æ–‡"
}}
"""

            try:
                ai_response = model.generate_content(prompt)
                if ai_response.text:
                    import json
                    import re

                    # JSONã‚’æŠ½å‡º
                    json_match = re.search(
                        r"\{.*\}", ai_response.text, re.DOTALL
                    )
                    if json_match:
                        response_data = json.loads(json_match.group())
                        feedback = response_data.get("feedback", "")
                        explanation = response_data.get("explanation", "")
                    else:
                        raise Exception("No JSON found in AI response")
                else:
                    raise Exception("Empty AI response")

            except Exception as e:
                print(f"AI feedback generation failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
                if is_correct:
                    feedback = "æ­£è§£ã§ã™ï¼ã‚ˆãã§ãã¾ã—ãŸã€‚"
                    explanation = f"ç­”ãˆã¯ã€Œ{req.correct_answer}ã€ã§ã™ã€‚"
                else:
                    feedback = (
                        f"æƒœã—ã„ï¼æ­£è§£ã¯ã€Œ{req.correct_answer}ã€ã§ã—ãŸã€‚"
                    )
                    explanation = "æ¬¡å›ã‚‚é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼"
        else:
            # Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            if is_correct:
                feedback = "æ­£è§£ã§ã™ï¼ç´ æ™´ã‚‰ã—ã„ï¼"
                explanation = f"ç­”ãˆã¯ã€Œ{req.correct_answer}ã€ã§ã™ã€‚"
            else:
                feedback = (
                    f"ä¸æ­£è§£ã§ã™ã€‚æ­£è§£ã¯ã€Œ{req.correct_answer}ã€ã§ã—ãŸã€‚"
                )
                explanation = "æ¬¡å›ã‚‚é ‘å¼µã£ã¦ãã ã•ã„ï¼"

        return ListeningAnswerResponse(
            is_correct=is_correct, feedback=feedback, explanation=explanation
        )

    except Exception as e:
        print(f"Error checking listening answer: {str(e)}")
        return ListeningAnswerResponse(
            is_correct=False,
            feedback="å›ç­”ã®ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            explanation="ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        )


# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œç¿»è¨³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================

@app.post("/api/listening/translate", response_model=ListeningTranslateResponse)
async def translate_listening_question(req: ListeningTranslateRequest):
    """
    ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®è‹±èªæ–‡ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Args:
        req: ç¿»è¨³ã™ã‚‹è‹±èªã®å•é¡Œæ–‡

    Returns:
        ListeningTranslateResponse: æ—¥æœ¬èªç¿»è¨³
    """
    try:
        if not model:
            # Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return ListeningTranslateResponse(
                japanese_translation="ç¿»è¨³æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
            )

        # Gemini AIã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³
        translate_prompt = f"""
ã‚ãªãŸã¯è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è‹±èªã®å•é¡Œæ–‡ã‚’è‡ªç„¶ã§ç†è§£ã—ã‚„ã™ã„æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

è‹±èªã®å•é¡Œæ–‡: {req.question}

ç¿»è¨³ã®è¦ä»¶:
1. è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èª
2. å•é¡Œã®æ„å‘³ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹
3. æ—¥æœ¬èªå­¦ç¿’è€…ã«ã¨ã£ã¦ç†è§£ã—ã‚„ã™ã„è¡¨ç¾
4. 1ã¤ã®å®Œçµã—ãŸæ–‡ç« ã§å›ç­”

å›ç­”å½¢å¼: ç¿»è¨³ã•ã‚ŒãŸæ—¥æœ¬èªã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""

        try:
            ai_response = model.generate_content(translate_prompt)
            if ai_response.text:
                japanese_translation = ai_response.text.strip()
            else:
                raise Exception("Empty AI response")

        except Exception as e:
            print(f"AI translation failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¿»è¨³
            japanese_translation = f"å•é¡Œæ–‡: {req.question}ï¼ˆç¿»è¨³æº–å‚™ä¸­ï¼‰"

        return ListeningTranslateResponse(
            japanese_translation=japanese_translation
        )

    except Exception as e:
        print(f"Error translating listening question: {str(e)}")
        return ListeningTranslateResponse(
            japanese_translation="ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        )


@app.post(
    "/api/instant-translation/check",
    response_model=InstantTranslationCheckResponse,
)
async def check_instant_translation_answer(
    req: InstantTranslationCheckRequest,
):
    """
    ç¬é–“è‹±ä½œæ–‡ã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’æ­£è§£ã¨æ¯”è¼ƒã—ã€AIã‚’ä½¿ã£ã¦è©³ç´°ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    print(f"ğŸ”” Instant translation check request: '{req.userAnswer[:30]}...'")

    try:
        if not model:
            # Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ¯”è¼ƒ
            is_correct = (
                req.userAnswer.lower().strip()
                == req.correctAnswer.lower().strip()
            )
            return InstantTranslationCheckResponse(
                isCorrect=is_correct,
                feedback=(
                    "Good try! Keep practicing."
                    if is_correct
                    else "Close, but not quite right. Try again!"
                ),
                score=100 if is_correct else 50,
                suggestions=[],
            )

        # AIã‚’ä½¿ã£ã¦è©³ç´°ãªå›ç­”ãƒã‚§ãƒƒã‚¯ï¼ˆéåŒæœŸå®Ÿè¡Œï¼‰
        check_prompt = create_translation_check_prompt(
            req.japanese, req.correctAnswer, req.userAnswer
        )

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(check_prompt)
        )

        if response.text:
            # AIå¿œç­”ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            ai_feedback = response.text

            # ç°¡å˜ãªæ­£è§£åˆ¤å®šï¼ˆAIã®å¿œç­”ã«åŸºã¥ãï¼‰
            is_correct = any(
                word in ai_feedback.lower()
                for word in ["correct", "good", "excellent", "right"]
            )

            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
            score = 100 if is_correct else 70

            return InstantTranslationCheckResponse(
                isCorrect=is_correct,
                feedback=ai_feedback,
                score=score,
                suggestions=[],
            )
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return InstantTranslationCheckResponse(
                isCorrect=False,
                feedback="Sorry, I couldn't evaluate your answer properly. Please try again.",
                score=50,
                suggestions=[],
            )

    except Exception as e:
        print(f"Error checking instant translation answer: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="Failed to check instant translation answer",
        )


@app.get(
    "/api/eiken-translation-problem",
    response_model=InstantTranslationProblem,
)
async def get_eiken_translation_problem(
    difficulty: str = "all", category: str = "all", eiken_level: str = ""
):
    """
    è‹±æ¤œå¯¾å¿œç¬é–“è‹±ä½œæ–‡å•é¡Œå–å¾—APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    /api/instant-translation/problemã¨åŒã˜æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

    Args:
        difficulty: å•é¡Œã®é›£æ˜“åº¦ (all, basic, intermediate, advanced)
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (all, daily_life, work, travel, etc.)
        eiken_level: è‹±æ¤œãƒ¬ãƒ™ãƒ« (5, 4, 3, pre-2, 2, pre-1, 1)
    """

    # æ—¢å­˜ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹
    return await get_instant_translation_problem(
        difficulty, category, eiken_level, False
    )


def create_eiken_problem_generation_prompt(
    eiken_level: str, category: str = "general", long_text_mode: bool = False
) -> str:
    """
    è‹±æ¤œãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸç¬é–“è‹±ä½œæ–‡å•é¡Œã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ

    Args:
        eiken_level: è‹±æ¤œãƒ¬ãƒ™ãƒ« (5, 4, 3, pre-2, 2, pre-1, 1)
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (daily_life, work, travel, etc.)

    Returns:
        AIãŒå•é¡Œã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """

    # è‹±æ¤œãƒ¬ãƒ™ãƒ«åˆ¥ã®ç‰¹å¾´å®šç¾©
    eiken_characteristics = {
        "5": {
            "description": "è‹±æ¤œ5ç´š (ä¸­å­¦åˆç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "ç¾åœ¨å½¢ã€éå»å½¢ã€beå‹•è©ã€ä¸€èˆ¬å‹•è©ã®åŸºæœ¬å½¢",
            "vocabulary": "ä¸­å­¦1å¹´ç”Ÿãƒ¬ãƒ™ãƒ«ã®åŸºæœ¬èªå½™ (ç´„600èª)",
            "sentence_structure": "ã‚·ãƒ³ãƒ—ãƒ«ãªå˜æ–‡ä¸­å¿ƒ",
            "examples": [
                "I am a student.",
                "I go to school.",
                "It is sunny today.",
            ],
        },
        "4": {
            "description": "è‹±æ¤œ4ç´š (ä¸­å­¦ä¸­ç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "åŠ©å‹•è© (can, will, must)ã€æœªæ¥å½¢ã€é€²è¡Œå½¢",
            "vocabulary": "ä¸­å­¦2å¹´ç”Ÿãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„1300èª)",
            "sentence_structure": "åŠ©å‹•è©ã‚’å«ã‚€æ–‡ã€ç–‘å•æ–‡ãƒ»å¦å®šæ–‡",
            "examples": [
                "I can play tennis.",
                "Will you help me?",
                "She is reading a book.",
            ],
        },
        "3": {
            "description": "è‹±æ¤œ3ç´š (ä¸­å­¦å’æ¥­ãƒ¬ãƒ™ãƒ«)",
            "grammar": "å—å‹•æ…‹ã€ç¾åœ¨å®Œäº†ã€ä¸å®šè©ã€å‹•åè©",
            "vocabulary": "ä¸­å­¦3å¹´ç”Ÿãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„2100èª)",
            "sentence_structure": "è¤‡æ–‡æ§‹é€ ã€æ¥ç¶šè©ã‚’ä½¿ã£ãŸæ–‡",
            "examples": [
                "This book was written by him.",
                "I have been to Tokyo.",
                "I want to learn English.",
            ],
        },
        "pre-2": {
            "description": "è‹±æ¤œæº–2ç´š (é«˜æ ¡ä¸­ç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "é–¢ä¿‚ä»£åè©ã€ä»®å®šæ³•ã®åŸºæœ¬ã€åˆ†è©",
            "vocabulary": "é«˜æ ¡åŸºç¤ãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„3600èª)",
            "sentence_structure": "é–¢ä¿‚è©ã‚’ä½¿ã£ãŸè¤‡æ–‡ã€ã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ ",
            "examples": [
                "The man who is standing there is my teacher.",
                "If I were you, I would study harder.",
            ],
        },
        "2": {
            "description": "è‹±æ¤œ2ç´š (é«˜æ ¡å’æ¥­ãƒ¬ãƒ™ãƒ«)",
            "grammar": "ä»®å®šæ³•ã€è¤‡é›‘ãªæ™‚åˆ¶ã€é«˜åº¦ãªæ–‡å‹",
            "vocabulary": "é«˜æ ¡å’æ¥­ãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„5100èª)",
            "sentence_structure": "è¤‡é›‘ãªè¤‡æ–‡ã€è«–ç†çš„ãªæ–‡æ§‹é€ ",
            "examples": [
                "If I had studied harder, I could have passed the exam.",
                "Having finished my homework, I went to bed.",
            ],
        },
        "pre-1": {
            "description": "è‹±æ¤œæº–1ç´š (å¤§å­¦ä¸­ç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "é«˜åº¦ãªæ–‡æ³•æ§‹é€ ã€è«–ç†çš„è¡¨ç¾",
            "vocabulary": "å¤§å­¦ä¸­ç´šãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„7500èª)",
            "sentence_structure": "å­¦è¡“çš„ãƒ»ãƒ“ã‚¸ãƒã‚¹çš„è¡¨ç¾",
            "examples": [
                "The proposal is likely to be implemented next year.",
                "It is essential that we address this issue promptly.",
            ],
        },
        "1": {
            "description": "è‹±æ¤œ1ç´š (å¤§å­¦ä¸Šç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®æ–‡æ³•ã€æ…£ç”¨è¡¨ç¾",
            "vocabulary": "å¤§å­¦ä¸Šç´šãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„10000-15000èª)",
            "sentence_structure": "é«˜åº¦ãªè«–ç†æ§‹é€ ã€å°‚é–€çš„è¡¨ç¾",
            "examples": [
                "The ramifications of this decision could be far-reaching.",
                "Notwithstanding the challenges, we must persevere.",
            ],
        },
    }

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯
    category_topics = {
        "daily_life": ["å®¶æ—", "é£Ÿäº‹", "è²·ã„ç‰©", "è¶£å‘³", "å¤©æ°—"],
        "work": ["ä»•äº‹", "ä¼šè­°", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", "åŒåƒš", "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"],
        "travel": ["æ—…è¡Œ", "äº¤é€š", "å®¿æ³Š", "è¦³å…‰", "æ–‡åŒ–"],
        "education": ["å­¦æ ¡", "å‹‰å¼·", "è©¦é¨“", "å›³æ›¸é¤¨", "æˆæ¥­"],
        "health": ["å¥åº·", "ç—…æ°—", "é‹å‹•", "é£Ÿäº‹", "ç—…é™¢"],
        "technology": [
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼",
            "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³",
            "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ",
            "ã‚¢ãƒ—ãƒª",
            "SNS",
        ],
        "general": ["ä¸€èˆ¬çš„ãªè©±é¡Œ", "æ—¥å¸¸çš„ãªè¡¨ç¾", "åŸºæœ¬çš„ãªä¼šè©±"],
    }

    eiken_info = eiken_characteristics.get(
        eiken_level, eiken_characteristics["3"]
    )
    topics = category_topics.get(category, category_topics["general"])

    prompt = f"""
ã‚ãªãŸã¯è‹±æ¤œå¯¾ç­–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æ¡ä»¶ã«å¾“ã£ã¦ç¬é–“è‹±ä½œæ–‡ã®å•é¡Œã‚’1ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ãƒ¬ãƒ™ãƒ«ã€‘
{eiken_info['description']}

ã€æ–‡æ³•ãƒ¬ãƒ™ãƒ«ã€‘
{eiken_info['grammar']}

ã€èªå½™ãƒ¬ãƒ™ãƒ«ã€‘
{eiken_info['vocabulary']}

ã€æ–‡æ§‹é€ ã€‘
{eiken_info['sentence_structure']}

ã€å‚è€ƒä¾‹æ–‡ã€‘
{', '.join(eiken_info['examples'])}

ã€å•é¡Œã‚«ãƒ†ã‚´ãƒªã€‘
{category} - ãƒˆãƒ”ãƒƒã‚¯ä¾‹: {', '.join(topics)}

ã€ä½œæˆæ¡ä»¶ã€‘
1. æŒ‡å®šã•ã‚ŒãŸè‹±æ¤œãƒ¬ãƒ™ãƒ«ã«é©ã—ãŸèªå½™ãƒ»æ–‡æ³•ã®ã¿ã‚’ä½¿ç”¨
2. æ—¥æœ¬äººå­¦ç¿’è€…ã«ã¨ã£ã¦å®Ÿç”¨æ€§ã®é«˜ã„è¡¨ç¾
3. è‡ªç„¶ã§é©åˆ‡ãªè‹±èªè¡¨ç¾
4. æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«é–¢é€£ã™ã‚‹å†…å®¹
5. {"è¤‡æ•°æ–‡ã§æ§‹æˆã™ã‚‹ï¼ˆé•·æ–‡ãƒ¢ãƒ¼ãƒ‰ï¼‰" if long_text_mode else "çŸ­ã„1æ–‡ã®ã¿ã§æ§‹æˆã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
    "japanese": "æ—¥æœ¬èªã®æ–‡ç« ",
    "english": "å¯¾å¿œã™ã‚‹è‹±èªã®æ–‡ç« ",
    "difficulty": "easy/medium/hard",
    "category": "ã‚«ãƒ†ã‚´ãƒªå"
}}

{"2-3æ–‡ã‹ã‚‰ãªã‚‹" if long_text_mode else "1ã¤ã®"}å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""

    return prompt
