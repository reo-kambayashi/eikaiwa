"""
FastAPI backend for the English Communication App.

This backend server provides AI-powered English conversation practice
for Japanese learners using Google Gemini AI and Google Cloud TTS.

Key features:
- AI conversation responses using Gemini
- Text-to-speech synthesis for pronunciation practice
- Grammar checking and feedback
- Voice input and output customization
"""

import asyncio
import base64
import os
from concurrent.futures import ThreadPoolExecutor

import google.generativeai as genai
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel

# Load environment variables from the `.env` file located at the project root.
# This allows developers to keep API keys outside of the source code for security.
# The load_dotenv() function automatically reads the .env file from the project root.
load_dotenv()

# API keys and credentials read from environment variables
# These are set in the .env file and loaded at runtime
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv(
    "GOOGLE_APPLICATION_CREDENTIALS", ""
)

# Configure Gemini AI model for conversation generation
# The gemini-2.5-flash model provides fast, high-quality responses
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel("gemini-2.5-flash")
    # Initialize Gemini TTS model
    tts_model = genai.GenerativeModel("gemini-2.5-flash-preview-tts")
else:
    model = None
    tts_model = None

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’è¨­å®šï¼ˆAIå‡¦ç†ã‚’éåŒæœŸåŒ–ã™ã‚‹ãŸã‚ï¼‰
executor = ThreadPoolExecutor(max_workers=4)

# ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”¨ã®é«˜é€ŸåŒ–ï¼‰
response_cache = {}
CACHE_TTL = 300  # 5åˆ†é–“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥

# Create FastAPI application instance
app = FastAPI()

# Add CORS middleware to allow frontend connections from React development server
# This is necessary for the frontend (localhost:3000) to communicate with backend (localhost:8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods (GET, POST, etc.)
    allow_headers=["*"],  # Allow all headers
)
# Pydantic models define the structure of requests and responses
# These ensure type safety and automatic validation


class Request(BaseModel):
    """
    Request model for conversation API calls from the frontend.

    This defines what data the frontend must send when requesting
    an AI response for English conversation practice.
    """

    text: str  # The user's input text or speech transcription
    conversation_history: list = []  # Previous messages for context
    enable_grammar_check: bool = True  # Whether to enable grammar checking


class JapaneseConsultationRequest(BaseModel):
    """
    Request model for Japanese consultation and dictionary features.
    
    For asking questions about English expressions, grammar, or vocabulary
    with responses in Japanese.
    """
    
    text: str  # User's question in Japanese or English
    conversation_history: list = []  # Previous consultation messages


class Response(BaseModel):
    """
    Response model returned by the API to the frontend.

    Contains the AI's response text that will be displayed
    and potentially converted to speech.
    """

    reply: str  # The AI's response text


class TTSRequest(BaseModel):
    """
    Request model for text-to-speech synthesis.

    Defines parameters for converting text to natural-sounding speech
    using Gemini 2.5 Flash Preview TTS service.
    """

    text: str  # Text to convert to speech
    voice_name: str = (
        "Kore"  # Default: bright female English voice for Gemini TTS
    )
    language_code: str = "en-US"  # Language and region code
    speaking_rate: float = 1.0  # Speech speed (0.25-4.0, 1.0 = normal)


class InstantTranslationCheckRequest(BaseModel):
    """
    Request model for instant translation answer checking.

    Defines the structure for checking user answers against correct translations
    in the instant translation mode.
    """

    japanese: str  # Original Japanese text
    correctAnswer: str  # Correct English translation
    userAnswer: str  # User's English translation attempt


class InstantTranslationProblem(BaseModel):
    """
    Response model for instant translation problems.

    Contains a Japanese sentence to be translated to English.
    """

    japanese: str  # Japanese sentence to translate
    english: str  # Correct English translation
    difficulty: str = "medium"  # Problem difficulty level
    category: str = "general"  # Grammar or topic category


class InstantTranslationCheckResponse(BaseModel):
    """
    Response model for instant translation answer checking.

    Contains evaluation results and feedback for the user's translation attempt.
    """

    isCorrect: bool  # Whether the answer is correct
    feedback: str  # Detailed feedback on the translation
    score: int  # Numerical score (0-100)
    suggestions: list = []  # Alternative translations or improvements


class CombinedResponse(BaseModel):
    """
    Combined response model for simultaneous text and audio generation.

    Contains both AI text response and TTS audio data for optimized performance.
    """

    reply: str  # The AI's text response
    audio_data: str = ""  # Base64 encoded audio data
    content_type: str = "text/plain"  # Audio MIME type
    use_browser_tts: bool = False  # Whether to fallback to browser TTS
    fallback_text: str = ""  # Text for browser TTS fallback
    processing_time: float = 0.0  # Total processing time in seconds


# API Endpoints
# These endpoints handle communication between the frontend and backend

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
import time


@app.get("/")
async def root():
    """
    Health check endpoint - confirms the API server is running.

    This is useful for monitoring and debugging server status.
    Returns a simple JSON message when the server is operational.
    """
    return {"message": "English Communication App API is running"}


@app.get("/health")
async def health_check():
    """
    Health check endpoint for Docker container monitoring.

    This endpoint is used by Docker's HEALTHCHECK instruction
    to verify that the application is running properly.

    Returns:
        dict: Simple status message indicating the service is healthy
    """
    return {"status": "healthy", "service": "eikaiwa-backend"}


@app.get("/api/status")
async def api_status():
    """
    Configuration status endpoint - checks if required services are available.

    Returns the status of:
    - Gemini AI API (for conversation generation)
    - Google Cloud credentials (for authentication)
    - TTS service (for speech synthesis)

    This helps users troubleshoot configuration issues.
    """
    return {
        "gemini_configured": bool(GEMINI_API_KEY and model),
        "google_credentials_configured": bool(
            GOOGLE_APPLICATION_CREDENTIALS
            and os.path.exists(GOOGLE_APPLICATION_CREDENTIALS)
        ),
        "gemini_tts_configured": bool(GEMINI_API_KEY and tts_model),
        "tts_configured": bool(tts_model),
    }


@app.post("/api/tts")
async def text_to_speech(request: TTSRequest):
    """Convert text to speech using Gemini TTS."""

    if not tts_model:
        raise HTTPException(
            status_code=503, detail="TTS service not available"
        )

    try:
        # TTSã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        import time

        tts_cache_key = f"tts_{hash(request.text)}_{request.voice_name}_{request.speaking_rate}"
        if tts_cache_key in response_cache:
            cached_data, timestamp = response_cache[tts_cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"âœ… TTS Cache hit for: {request.text[:30]}...")
                return cached_data

        # Gemini 2.5 Flash Preview TTS with dictionary-based config
        content = request.text

        # Configure generation with dictionary format
        generation_config = {
            "response_modalities": ["AUDIO"],
            "speech_config": {
                "voice_config": {
                    "prebuilt_voice_config": {"voice_name": request.voice_name}
                }
            },
        }

        # Generate audio using Gemini TTS model (éåŒæœŸå®Ÿè¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor,
            lambda: tts_model.generate_content(
                contents=content, generation_config=generation_config
            ),
        )

        # Extract audio data from response
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                for part in candidate.content.parts:
                    if hasattr(part, "inline_data") and part.inline_data:
                        # Found audio data - extract properly
                        audio_data = part.inline_data.data
                        mime_type = part.inline_data.mime_type or "audio/wav"

                        print(
                            f"ğŸµ Audio data found: type={type(audio_data)}, mime_type={mime_type}"
                        )

                        # Handle different data types from Gemini
                        if isinstance(audio_data, bytes):
                            # If bytes, encode to base64
                            audio_base64 = base64.b64encode(audio_data).decode(
                                "utf-8"
                            )
                            print(
                                f"ğŸ”„ Encoded bytes to base64: {len(audio_data)} bytes -> {len(audio_base64)} chars"
                            )
                        elif isinstance(audio_data, str):
                            # If already string, assume it's base64
                            audio_base64 = audio_data
                            print(
                                f"ğŸ”„ Using string as base64: {len(audio_base64)} chars"
                            )
                        else:
                            print(
                                f"âŒ Unexpected audio data type: {type(audio_data)}"
                            )
                            raise ValueError(
                                f"Unexpected audio data type: {type(audio_data)}"
                            )

                        # Validate base64 data
                        try:
                            # Test decode to verify it's valid base64
                            decoded_test = base64.b64decode(audio_base64)
                            print(
                                f"âœ… Base64 validation successful: {len(decoded_test)} bytes decoded"
                            )
                        except Exception as b64_error:
                            print(f"âŒ Base64 validation failed: {b64_error}")
                            raise ValueError(
                                f"Invalid base64 audio data: {b64_error}"
                            )

                        result = {
                            "audio_data": audio_base64,
                            "content_type": mime_type,
                            "original_size": (
                                len(audio_data)
                                if isinstance(audio_data, (bytes, str))
                                else 0
                            ),
                        }
                        # TTSãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                        response_cache[tts_cache_key] = (result, time.time())
                        return result

        # If no audio data found, fallback to browser TTS
        print("No audio data found in Gemini TTS response")
        fallback_result = {
            "audio_data": "",
            "content_type": "text/plain",
            "fallback_text": request.text,
            "use_browser_tts": True,
        }
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        response_cache[tts_cache_key] = (fallback_result, time.time())
        return fallback_result

    except HTTPException:
        # Propagate HTTP errors such as 503 without modification
        raise
    except Exception as e:
        print(f"Gemini TTS Error: {str(e)}")
        # Fallback to browser TTS
        error_result = {
            "audio_data": "",
            "content_type": "text/plain",
            "fallback_text": request.text,
            "use_browser_tts": True,
            "error": str(e),
        }
        # ã‚¨ãƒ©ãƒ¼çµæœã¯çŸ­æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ30ç§’ï¼‰
        response_cache[tts_cache_key] = (
            error_result,
            time.time() - CACHE_TTL + 30,
        )
        return error_result


@app.get("/api/welcome", response_model=Response)
async def get_welcome_message():
    """Generate a personalized welcome message."""

    print("ğŸ”” Welcome request received")

    try:
        if not model:
            return Response(
                reply="Hello! Welcome to English Communication App! Please set up your API key to get started."
            )

        welcome_prompt = create_welcome_prompt()
        # AIç”Ÿæˆã‚’éåŒæœŸå®Ÿè¡Œ
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(welcome_prompt)
        )

        if response.text:
            return Response(reply=response.text)
        else:
            return Response(
                reply="Hello! Welcome to English Communication App! Let's start practicing English together!"
            )

    except Exception as e:
        print(f"Error generating welcome message: {str(e)}")
        return Response(
            reply="Hello! Welcome to English Communication App! I'm here to help you practice English. How are you today?"
        )


@app.post("/api/respond", response_model=Response)
async def respond(req: Request):
    """Generate a response using Gemini API for English conversation practice."""

    print(f"ğŸ”” Response request received: text='{req.text[:50]}...'")

    try:
        if not model:
            # Fallback response if Gemini API is not configured
            return Response(
                reply="API key not configured. Please set GEMINI_API_KEY environment variable."
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = (
            f"response_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )
        import time

        if cache_key in response_cache:
            cached_data, timestamp = response_cache[cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"âœ… Cache hit for response: {req.text[:30]}...")
                return Response(reply=cached_data)

        # Create conversation prompt
        prompt = create_conversation_prompt(req.text, req.conversation_history)

        # Generate response using Gemini (éåŒæœŸå®Ÿè¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        if response.text:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            response_cache[cache_key] = (response.text, time.time())
            return Response(reply=response.text)
        else:
            return Response(
                reply="Sorry, I couldn't generate a response. Please try again."
            )

    except Exception as e:
        # Log the error in production, but don't expose internal details
        print(f"Error generating response: {str(e)}")
        return Response(
            reply="Sorry, there was an error processing your request. Please try again."
        )


@app.post("/api/japanese-consultation", response_model=Response)
async def japanese_consultation(req: JapaneseConsultationRequest):
    """Generate Japanese consultation response for English expression and grammar questions."""

    print(f"ğŸ”” Japanese consultation request: text='{req.text[:50]}...'")

    try:
        if not model:
            # Fallback response if Gemini API is not configured
            return Response(
                reply="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = (
            f"consultation_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )
        import time

        if cache_key in response_cache:
            cached_data, timestamp = response_cache[cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"âœ… Cache hit for consultation: {req.text[:30]}...")
                return Response(reply=cached_data)

        # Create Japanese consultation prompt
        prompt = create_japanese_consultation_prompt(
            req.text, "general", req.conversation_history
        )

        # Generate response using Gemini (éåŒæœŸå®Ÿè¡Œ)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        if response.text:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            response_cache[cache_key] = (response.text, time.time())
            return Response(reply=response.text)
        else:
            return Response(
                reply="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            )

    except Exception as e:
        # Log the error in production, but don't expose internal details
        print(f"Error generating Japanese consultation response: {str(e)}")
        return Response(
            reply="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        )


@app.post("/api/respond-with-audio", response_model=CombinedResponse)
async def respond_with_audio(
    req: Request, voice_name: str = "Kore", speaking_rate: float = 1.0
):
    """
    Generate both text response and audio simultaneously for optimal performance.

    This endpoint combines conversation generation and TTS processing
    to reduce total response time for single-user scenarios.
    """

    print(
        f"ğŸ”” Combined response request: text='{req.text[:50]}...', voice={voice_name}"
    )
    start_time = time.time()

    try:
        if not model:
            return CombinedResponse(
                reply="API key not configured. Please set GEMINI_API_KEY environment variable.",
                use_browser_tts=True,
                fallback_text="API key not configured. Please set GEMINI_API_KEY environment variable.",
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        import time

        cache_key = (
            f"response_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        prompt = create_conversation_prompt(req.text, req.conversation_history)
        loop = asyncio.get_event_loop()

        # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã‚’éåŒæœŸå®Ÿè¡Œ
        response_future = loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¾…ã¤
        ai_response = await response_future

        if not ai_response.text:
            return CombinedResponse(
                reply="Sorry, I couldn't generate a response. Please try again.",
                use_browser_tts=True,
                fallback_text="Sorry, I couldn't generate a response. Please try again.",
            )

        reply_text = ai_response.text

        # TTSç”Ÿæˆã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆAIãƒ¬ã‚¹ãƒãƒ³ã‚¹å¾Œï¼‰
        if tts_model:
            tts_cache_key = (
                f"tts_{hash(reply_text)}_{voice_name}_{speaking_rate}"
            )

            # TTSã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if tts_cache_key in response_cache:
                cached_tts, timestamp = response_cache[tts_cache_key]
                if time.time() - timestamp < CACHE_TTL:
                    print(f"âœ… TTS Cache hit for combined response")
                    processing_time = time.time() - start_time
                    return CombinedResponse(
                        reply=reply_text,
                        audio_data=cached_tts.get("audio_data", ""),
                        content_type=cached_tts.get(
                            "content_type", "text/plain"
                        ),
                        use_browser_tts=cached_tts.get(
                            "use_browser_tts", False
                        ),
                        fallback_text=cached_tts.get(
                            "fallback_text", reply_text
                        ),
                        processing_time=processing_time,
                    )

            # TTSç”Ÿæˆè¨­å®š
            generation_config = {
                "response_modalities": ["AUDIO"],
                "speech_config": {
                    "voice_config": {
                        "prebuilt_voice_config": {"voice_name": voice_name}
                    }
                },
            }

            try:
                # TTSç”Ÿæˆã‚’éåŒæœŸå®Ÿè¡Œ
                tts_response = await loop.run_in_executor(
                    executor,
                    lambda: tts_model.generate_content(
                        contents=reply_text,
                        generation_config=generation_config,
                    ),
                )

                # TTSã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                audio_data = ""
                content_type = "text/plain"
                use_browser_tts = True

                if (
                    tts_response.candidates
                    and len(tts_response.candidates) > 0
                ):
                    candidate = tts_response.candidates[0]
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if (
                                hasattr(part, "inline_data")
                                and part.inline_data
                            ):
                                raw_audio = part.inline_data.data
                                content_type = (
                                    part.inline_data.mime_type or "audio/wav"
                                )

                                if isinstance(raw_audio, bytes):
                                    audio_data = base64.b64encode(
                                        raw_audio
                                    ).decode("utf-8")
                                elif isinstance(raw_audio, str):
                                    audio_data = raw_audio

                                if audio_data:
                                    use_browser_tts = False
                                    break

                # TTSçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                tts_result = {
                    "audio_data": audio_data,
                    "content_type": content_type,
                    "use_browser_tts": use_browser_tts,
                    "fallback_text": reply_text if use_browser_tts else "",
                }
                response_cache[tts_cache_key] = (tts_result, time.time())

                processing_time = time.time() - start_time
                print(
                    f"âš™ï¸ Combined processing completed in {processing_time:.2f}s"
                )

                return CombinedResponse(
                    reply=reply_text,
                    audio_data=audio_data,
                    content_type=content_type,
                    use_browser_tts=use_browser_tts,
                    fallback_text=reply_text if use_browser_tts else "",
                    processing_time=processing_time,
                )

            except Exception as tts_error:
                print(f"TTS Error in combined response: {str(tts_error)}")
                # TTSã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ–ãƒ©ã‚¦ã‚¶TTSã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                processing_time = time.time() - start_time
                return CombinedResponse(
                    reply=reply_text,
                    use_browser_tts=True,
                    fallback_text=reply_text,
                    processing_time=processing_time,
                )

        # TTSãƒ¢ãƒ‡ãƒ«ãŒç„¡ã„å ´åˆ
        processing_time = time.time() - start_time
        return CombinedResponse(
            reply=reply_text,
            use_browser_tts=True,
            fallback_text=reply_text,
            processing_time=processing_time,
        )

    except Exception as e:
        print(f"Error in combined response: {str(e)}")
        processing_time = time.time() - start_time
        return CombinedResponse(
            reply="Sorry, there was an error processing your request. Please try again.",
            use_browser_tts=True,
            fallback_text="Sorry, there was an error processing your request. Please try again.",
            processing_time=processing_time,
        )


def create_conversation_prompt(
    user_text: str, conversation_history: list = None
) -> str:
    """
    Create prompts for English conversation practice.

    Args:
        user_text: The user's input message
        conversation_history: Previous messages for context

    Returns:
        A formatted prompt string optimized for conversation practice
    """

    # Format conversation history for context
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        history_context = "\n\nCONVERSATION HISTORY (for context):\n"
        # Show last 10 messages to avoid token limit issues
        recent_history = (
            conversation_history[-10:]
            if len(conversation_history) > 10
            else conversation_history
        )
        for msg in recent_history:
            sender = msg.get("sender", "Unknown")
            text = msg.get("text", "")
            history_context += f"{sender}: {text}\n"
        history_context += "\n"

    # Simplified conversation prompt
    prompt = f"""
You are an expert English teacher and conversation partner specializing in helping Japanese learners.

IMPORTANT GUIDELINES:
- Always be encouraging and supportive
- Use natural, conversational English
- Provide gentle corrections when needed
- Ask follow-up questions to keep the conversation flowing
- Use examples and explanations when helpful
- Reference previous parts of the conversation when relevant
- Keep responses concise and engaging (1-3 sentences)
- Focus on practical, everyday English
{history_context}

CURRENT MESSAGE FROM STUDENT:
"{user_text}"

Please respond naturally as a friendly English teacher and conversation partner.
"""

    return prompt


def create_welcome_prompt() -> str:
    """Create a welcome prompt for new users."""

    prompt = """
You are an expert English teacher and conversation partner specializing in helping Japanese learners.

Please create a warm, encouraging welcome message for a new student starting English conversation practice.

GUIDELINES:
- Keep it friendly and encouraging
- Mention that you're here to help with English conversation
- Invite them to start practicing by asking a question or sharing something about themselves
- Keep it concise (2-3 sentences)
- Use clear, natural English

Please respond with a welcoming message to get the conversation started.
"""

    return prompt


def create_japanese_consultation_prompt(
    user_text: str, consultation_type: str = "general", conversation_history: list = None
) -> str:
    """
    Create prompts for Japanese consultation about English expressions and grammar.

    Args:
        user_text: The user's question in Japanese or English
        consultation_type: Type of consultation (kept for API compatibility)
        conversation_history: Previous consultation messages for context

    Returns:
        A formatted prompt string optimized for Japanese consultation responses
    """

    # Format conversation history for context
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        history_context = "\n\nç›¸è«‡å±¥æ­´ï¼ˆå‚è€ƒæƒ…å ±ï¼‰:\n"
        # Show last 8 messages to avoid token limit issues
        recent_history = (
            conversation_history[-8:]
            if len(conversation_history) > 8
            else conversation_history
        )
        for msg in recent_history:
            sender = msg.get("sender", "Unknown")
            text = msg.get("text", "")
            history_context += f"{sender}: {text}\n"
        history_context += "\n"

    # Simple Japanese consultation prompt
    prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬äººã®è‹±èªå­¦ç¿’è€…ã‚’å°‚é–€ã¨ã™ã‚‹ã€çµŒé¨“è±Šå¯Œã§è¦ªåˆ‡ãªè‹±èªæ•™å¸«ã§ã™ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘:
- å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ï¼ˆ2-3æ–‡ç¨‹åº¦ï¼‰
- 1ã¤ã®å…·ä½“çš„ãªä¾‹æ–‡ã‚’å«ã‚ã¦ãã ã•ã„
- ä¸€ç›®ã§èª­ã‚ã‚‹çŸ­ã•ã«ã—ã¦ãã ã•ã„
- è¦ç‚¹ã ã‘ã‚’ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„
{history_context}

ã€å­¦ç¿’è€…ã‹ã‚‰ã®è³ªå•ã€‘:
"{user_text}"

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ä¾‹æ–‡ã¯1ã¤ã ã‘ã€èª¬æ˜ã¯2-3æ–‡ä»¥å†…ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""

    return prompt


def optimize_cache_cleanup():
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ï¼‰
    """
    import time

    current_time = time.time()
    expired_keys = []

    for key, (data, timestamp) in response_cache.items():
        if current_time - timestamp > CACHE_TTL:
            expired_keys.append(key)

    for key in expired_keys:
        del response_cache[key]

    print(f"ğŸ§¹ Cache cleanup: removed {len(expired_keys)} expired entries")


# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®šæœŸå®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚¿ã‚¹ã‚¯
import threading
import time


def periodic_cache_cleanup():
    """å®šæœŸçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    while True:
        time.sleep(300)  # 5åˆ†æ¯ã«å®Ÿè¡Œ
        optimize_cache_cleanup()


# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹
cleanup_thread = threading.Thread(target=periodic_cache_cleanup, daemon=True)
cleanup_thread.start()


# ============================================================================
# ç¬é–“è‹±ä½œæ–‡ãƒ¢ãƒ¼ãƒ‰ç”¨ã®API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================

# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œãƒ¢ãƒ¼ãƒ‰ç”¨ã®API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================


# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å¿œç­”ãƒ¢ãƒ‡ãƒ«
class ListeningProblem(BaseModel):
    """
    ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
    """

    question: str  # å•é¡Œæ–‡ï¼ˆéŸ³å£°ã§èª­ã¿ä¸Šã’ã‚‹ï¼‰
    choices: list  # é¸æŠè‚¢ã®ãƒªã‚¹ãƒˆ
    correct_answer: str  # æ­£è§£
    difficulty: str  # é›£æ˜“åº¦ï¼ˆeasy, medium, hardï¼‰
    category: str  # ã‚«ãƒ†ã‚´ãƒª
    explanation: str  # è§£èª¬ï¼ˆä»»æ„ï¼‰


class ListeningAnswerRequest(BaseModel):
    """
    ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å›ç­”ãƒã‚§ãƒƒã‚¯ç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
    """

    question: str  # å•é¡Œæ–‡
    user_answer: str  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”
    correct_answer: str  # æ­£è§£
    choices: list  # é¸æŠè‚¢


class ListeningAnswerResponse(BaseModel):
    """
    ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å›ç­”ãƒã‚§ãƒƒã‚¯ç”¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
    """

    is_correct: bool  # æ­£è§£ã‹ã©ã†ã‹
    feedback: str  # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    explanation: str  # è§£èª¬


# ç¬é–“è‹±ä½œæ–‡ã®å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
TRANSLATION_PROBLEMS = [
    {
        "japanese": "ä»Šæ—¥ã¯å¤©æ°—ãŒã„ã„ã§ã™ã­ã€‚",
        "english": "It's nice weather today.",
        "difficulty": "easy",
        "category": "weather",
    },
    {
        "japanese": "æ˜¨æ—¥ã€å‹é”ã¨æ˜ ç”»ã‚’è¦‹ã«è¡Œãã¾ã—ãŸã€‚",
        "english": "I went to see a movie with my friend yesterday.",
        "difficulty": "medium",
        "category": "daily_life",
    },
    {
        "japanese": "æ¥é€±ã®é‡‘æ›œæ—¥ã«ä¼šè­°ãŒã‚ã‚Šã¾ã™ã€‚",
        "english": "There will be a meeting next Friday.",
        "difficulty": "medium",
        "category": "business",
    },
    {
        "japanese": "ã‚‚ã—ã‚‚æ™‚é–“ãŒã‚ã‚Œã°ã€ä¸€ç·’ã«è²·ã„ç‰©ã«è¡Œãã¾ã›ã‚“ã‹ï¼Ÿ",
        "english": "If you have time, would you like to go shopping together?",
        "difficulty": "hard",
        "category": "invitation",
    },
    {
        "japanese": "å½¼å¥³ã¯æ¯æœ7æ™‚ã«èµ·ãã¾ã™ã€‚",
        "english": "She gets up at 7 o'clock every morning.",
        "difficulty": "easy",
        "category": "daily_routine",
    },
    {
        "japanese": "ã“ã®æœ¬ã¯ç§ã«ã¨ã£ã¦é›£ã—ã™ãã¾ã™ã€‚",
        "english": "This book is too difficult for me.",
        "difficulty": "medium",
        "category": "opinion",
    },
    {
        "japanese": "é›»è»ŠãŒé…ã‚Œã¦ã„ã‚‹ã®ã§ã€å°‘ã—é…ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
        "english": "The train is delayed, so I might be a little late.",
        "difficulty": "hard",
        "category": "transportation",
    },
    {
        "japanese": "å¤ä¼‘ã¿ã«å®¶æ—ã¨åŒ—æµ·é“ã«è¡Œãäºˆå®šã§ã™ã€‚",
        "english": "I'm planning to go to Hokkaido with my family during summer vacation.",
        "difficulty": "medium",
        "category": "travel",
    },
    {
        "japanese": "æ—¥æœ¬èªã‚’å‹‰å¼·ã™ã‚‹ã®ã¯æ¥½ã—ã„ã§ã™ã€‚",
        "english": "Studying Japanese is fun.",
        "difficulty": "easy",
        "category": "learning",
    },
    {
        "japanese": "ã‚‚ã—é›¨ãŒé™ã£ãŸã‚‰ã€å®¶ã«ã„ã‚‹ã¤ã‚‚ã‚Šã§ã™ã€‚",
        "english": "If it rains, I intend to stay home.",
        "difficulty": "hard",
        "category": "conditional",
    },
    # è¿½åŠ ã®work/businessã‚«ãƒ†ã‚´ãƒªå•é¡Œ
    {
        "japanese": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—ã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ",
        "english": "How is the progress of the project?",
        "difficulty": "medium",
        "category": "work",
    },
    {
        "japanese": "æ¥æœˆã‹ã‚‰æ–°ã—ã„éƒ¨ç½²ã«ç•°å‹•ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã—ãŸã€‚",
        "english": "I will be transferred to a new department starting next month.",
        "difficulty": "hard",
        "category": "work",
    },
    {
        "japanese": "ã“ã®ææ¡ˆæ›¸ã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚",
        "english": "I have a question about this proposal.",
        "difficulty": "medium",
        "category": "work",
    },
    {
        "japanese": "ä¼šè­°ã®è³‡æ–™ã‚’æº–å‚™ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
        "english": "I need to prepare materials for the meeting.",
        "difficulty": "easy",
        "category": "work",
    },
    {
        "japanese": "ç· åˆ‡ã‚’å»¶é•·ã—ã¦ã„ãŸã ãã“ã¨ã¯å¯èƒ½ã§ã—ã‚‡ã†ã‹ï¼Ÿ",
        "english": "Would it be possible to extend the deadline?",
        "difficulty": "hard",
        "category": "work",
    },
    # technology ã‚«ãƒ†ã‚´ãƒª
    {
        "japanese": "æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚",
        "english": "I downloaded a new app.",
        "difficulty": "easy",
        "category": "technology",
    },
    {
        "japanese": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒå‹•ã‹ãªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚",
        "english": "My computer has stopped working.",
        "difficulty": "medium",
        "category": "technology",
    },
    {
        "japanese": "ã“ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯éå¸¸ã«ä½¿ã„ã‚„ã™ã„ã§ã™ã€‚",
        "english": "This software is very user-friendly.",
        "difficulty": "medium",
        "category": "technology",
    },
    # health ã‚«ãƒ†ã‚´ãƒª
    {
        "japanese": "é ­ãŒç—›ã„ã®ã§ç—…é™¢ã«è¡Œãã¾ã™ã€‚",
        "english": "I have a headache, so I'm going to the hospital.",
        "difficulty": "easy",
        "category": "health",
    },
    {
        "japanese": "æ¯æ—¥é‹å‹•ã™ã‚‹ã‚ˆã†ã«å¿ƒãŒã‘ã¦ã„ã¾ã™ã€‚",
        "english": "I try to exercise every day.",
        "difficulty": "medium",
        "category": "health",
    },
    {
        "japanese": "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿäº‹ã‚’æ‘‚ã‚‹ã“ã¨ãŒå¤§åˆ‡ã§ã™ã€‚",
        "english": "It's important to have a balanced diet.",
        "difficulty": "hard",
        "category": "health",
    },
    # education ã‚«ãƒ†ã‚´ãƒª
    {
        "japanese": "å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å°‚æ”»ã—ã¦ã„ã¾ã™ã€‚",
        "english": "I'm majoring in economics at university.",
        "difficulty": "medium",
        "category": "education",
    },
    {
        "japanese": "å›³æ›¸é¤¨ã§å®¿é¡Œã‚’ã—ã¦ã„ã¾ã™ã€‚",
        "english": "I'm doing my homework at the library.",
        "difficulty": "easy",
        "category": "education",
    },
    {
        "japanese": "ä»Šåº¦ã®è©¦é¨“ã®æº–å‚™ã‚’ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚",
        "english": "I have to prepare for the upcoming exam.",
        "difficulty": "medium",
        "category": "education",
    },
]


@app.get(
    "/api/instant-translation/problem",
    response_model=InstantTranslationProblem,
)
async def get_instant_translation_problem(
    difficulty: str = "all",
    category: str = "all",
    eiken_level: str = "",
    long_text_mode: bool = False,
):
    """
    ç¬é–“è‹±ä½œæ–‡ã®å•é¡Œã‚’å–å¾—ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    é›£æ˜“åº¦ã€ã‚«ãƒ†ã‚´ãƒªã€è‹±æ¤œãƒ¬ãƒ™ãƒ«ã«åŸºã¥ã„ã¦é©åˆ‡ãªå•é¡Œã‚’è¿”ã—ã¾ã™ã€‚
    è‹±æ¤œãƒ¬ãƒ™ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€AIã‚’ä½¿ã£ã¦å‹•çš„ã«å•é¡Œã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        difficulty: å•é¡Œã®é›£æ˜“åº¦ (all, basic, intermediate, advanced)
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (all, daily_life, work, travel, etc.)
        eiken_level: è‹±æ¤œãƒ¬ãƒ™ãƒ« (5, 4, 3, pre-2, 2, pre-1, 1)
    """

    print(
        f"ğŸ”” Instant translation problem request: difficulty={difficulty}, category={category}, eiken_level={eiken_level}, long_text_mode={long_text_mode}"
    )

    try:
        import json
        import random

        # è‹±æ¤œãƒ¬ãƒ™ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¦ã€AIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯AIç”Ÿæˆã‚’è©¦è¡Œ
        if eiken_level and eiken_level.strip() and model:
            print(f"ğŸ¤– Generating AI problem for Eiken level {eiken_level}")

            try:
                # ã‚«ãƒ†ã‚´ãƒªã®ãƒãƒƒãƒ”ãƒ³ã‚°
                category_for_ai = category if category != "all" else "general"

                # AIå•é¡Œç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                ai_prompt = create_eiken_problem_generation_prompt(
                    eiken_level, category_for_ai, long_text_mode
                )

                # AIã«å•é¡Œç”Ÿæˆã‚’ä¾é ¼ï¼ˆéåŒæœŸå®Ÿè¡Œï¼‰
                loop = asyncio.get_event_loop()
                ai_response = await loop.run_in_executor(
                    executor, lambda: model.generate_content(ai_prompt)
                )

                if ai_response.text:
                    # AIã®å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦JSONã‚’æŠ½å‡º
                    ai_text = ai_response.text.strip()

                    # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
                    json_start = ai_text.find("{")
                    json_end = ai_text.rfind("}") + 1

                    if json_start != -1 and json_end > json_start:
                        json_text = ai_text[json_start:json_end]

                        try:
                            ai_problem = json.loads(json_text)

                            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                            if all(
                                key in ai_problem
                                for key in ["japanese", "english"]
                            ):
                                print(f"âœ… AI generated problem successfully")

                                # é›£æ˜“åº¦ã¨ã‚«ãƒ†ã‚´ãƒªã‚’èª¿æ•´
                                eiken_to_difficulty = {
                                    "5": "easy",
                                    "4": "easy",
                                    "3": "medium",
                                    "pre-2": "medium",
                                    "2": "medium",
                                    "pre-1": "hard",
                                    "1": "hard",
                                }

                                return InstantTranslationProblem(
                                    japanese=ai_problem["japanese"],
                                    english=ai_problem["english"],
                                    difficulty=ai_problem.get(
                                        "difficulty",
                                        eiken_to_difficulty.get(
                                            eiken_level, "medium"
                                        ),
                                    ),
                                    category=ai_problem.get(
                                        "category", category_for_ai
                                    ),
                                )
                            else:
                                print(
                                    f"âš ï¸ AI response missing required fields, falling back to static problems"
                                )
                        except json.JSONDecodeError as e:
                            print(
                                f"âš ï¸ Failed to parse AI JSON response: {e}, falling back to static problems"
                            )
                    else:
                        print(
                            f"âš ï¸ No valid JSON found in AI response, falling back to static problems"
                        )
                else:
                    print(
                        f"âš ï¸ Empty AI response, falling back to static problems"
                    )

            except Exception as e:
                print(
                    f"âš ï¸ AI problem generation failed: {e}, falling back to static problems"
                )

        # é™çš„å•é¡Œãƒªã‚¹ãƒˆã‹ã‚‰ã®é¸æŠï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print(f"ğŸ“š Using static problem list")

        # è‹±æ¤œãƒ¬ãƒ™ãƒ«ã‚’é›£æ˜“åº¦ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        eiken_to_difficulty = {
            "5": "easy",
            "4": "easy",
            "3": "medium",
            "pre-2": "medium",
            "2": "medium",
            "pre-1": "hard",
            "1": "hard",
        }

        # é›£æ˜“åº¦ã®æ±ºå®š - è‹±æ¤œãƒ¬ãƒ™ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆ
        if eiken_level and eiken_level in eiken_to_difficulty:
            target_difficulty = eiken_to_difficulty[eiken_level]
        elif difficulty != "all":
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®é›£æ˜“åº¦ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å½¢å¼ã«å¤‰æ›
            difficulty_mapping = {
                "basic": "easy",
                "intermediate": "medium",
                "advanced": "hard",
            }
            target_difficulty = difficulty_mapping.get(difficulty, "medium")
        else:
            target_difficulty = "all"

        # å•é¡Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_problems = []

        # é›£æ˜“åº¦ãƒ•ã‚£ãƒ«ã‚¿
        if target_difficulty == "all":
            filtered_problems = TRANSLATION_PROBLEMS.copy()
        else:
            filtered_problems = [
                p
                for p in TRANSLATION_PROBLEMS
                if p["difficulty"] == target_difficulty
            ]

        # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
        if category != "all":
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ã‚«ãƒ†ã‚´ãƒªåã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å½¢å¼ã«å¤‰æ›
            category_mapping = {
                "daily_life": ["daily_life", "daily_routine", "preferences"],
                "work": ["business", "work"],
                "travel": ["travel", "transportation"],
                "education": ["learning", "education"],
                "technology": ["technology"],
                "health": ["health"],
                "culture": ["general"],  # ä»Šå¾Œè¿½åŠ äºˆå®š
                "environment": ["general"],  # ä»Šå¾Œè¿½åŠ äºˆå®š
            }

            target_categories = category_mapping.get(category, [category])
            filtered_problems = [
                p
                for p in filtered_problems
                if p["category"] in target_categories
            ]
        # åˆ©ç”¨å¯èƒ½ãªå•é¡ŒãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not filtered_problems:
            print(f"No problems found for filters, using fallback")
            filtered_problems = TRANSLATION_PROBLEMS.copy()

        # ãƒ©ãƒ³ãƒ€ãƒ ã«å•é¡Œã‚’é¸æŠ
        problem = random.choice(filtered_problems)

        return InstantTranslationProblem(
            japanese=problem["japanese"],
            english=problem["english"],
            difficulty=problem["difficulty"],
            category=problem["category"],
        )

    except Exception as e:
        print(f"Error generating instant translation problem: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å•é¡Œ
        fallback_problem = {
            "japanese": "ç§ã¯æ¯æ—¥è‹±èªã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚",
            "english": "I study English every day.",
            "difficulty": "easy",
            "category": "daily_life",
        }

        return InstantTranslationProblem(
            japanese=fallback_problem["japanese"],
            english=fallback_problem["english"],
            difficulty=fallback_problem["difficulty"],
            category=fallback_problem["category"],
        )


# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œå–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================


@app.get("/api/listening/problem", response_model=ListeningProblem)
async def get_listening_problem(
    category: str = "any",
    difficulty: str = "medium",
    _t: str = None,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°ç”¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
):
    """
    Trivia APIã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã‚’å–å¾—ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Args:
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (any, sports, science, history, etc.)
        difficulty: é›£æ˜“åº¦ (easy, medium, hard)
        _t: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°ç”¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆå†…éƒ¨ã§ã¯ä½¿ç”¨ã—ãªã„ï¼‰

    Returns:
        ListeningProblem: å•é¡Œæ–‡ã€é¸æŠè‚¢ã€æ­£è§£ã€é›£æ˜“åº¦ã€ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€
    """
    try:
        import httpx

        # Open Trivia Database APIã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        base_url = "https://opentdb.com/api.php"
        params = {
            "amount": 1,  # 1å•å–å¾—
            "type": "multiple",  # å¤šè‚¢é¸æŠå•é¡Œ
            "difficulty": difficulty,
            "encode": "url3986",  # RFC 3986 URL ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        }

        # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆOpen Trivia DBã®ã‚«ãƒ†ã‚´ãƒªIDï¼‰
        category_mapping = {
            "any": None,
            "general": 9,
            "books": 10,
            "film": 11,
            "music": 12,
            "television": 14,
            "science": 17,
            "computers": 18,
            "math": 19,
            "mythology": 20,
            "sports": 21,
            "geography": 22,
            "history": 23,
            "politics": 24,
            "art": 25,
            "celebrities": 26,
            "animals": 27,
            "vehicles": 28,
        }

        # ã‚«ãƒ†ã‚´ãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
        if category != "any" and category in category_mapping:
            params["category"] = category_mapping[category]

        # Trivia APIã‹ã‚‰å•é¡Œã‚’å–å¾—ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™è€ƒæ…®ï¼‰
        import asyncio
        import time

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ5ç§’é–“éš”ï¼‰
        current_time = time.time()
        if hasattr(get_listening_problem, "_last_request_time"):
            time_since_last = (
                current_time - get_listening_problem._last_request_time
            )
            if time_since_last < 5.0:
                wait_time = 5.0 - time_since_last
                print(f"â³ Rate limit: waiting {wait_time:.1f} seconds")
                await asyncio.sleep(wait_time)

        get_listening_problem._last_request_time = time.time()

        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        response_code = data.get("response_code", -1)

        if response_code == 1:
            raise Exception(
                "API Error: Not enough questions for the specified criteria"
            )
        elif response_code == 2:
            raise Exception("API Error: Invalid parameters")
        elif response_code == 3:
            raise Exception("API Error: Token not found")
        elif response_code == 4:
            raise Exception("API Error: Token exhausted")
        elif response_code == 5:
            raise Exception("API Error: Rate limit exceeded")
        elif response_code != 0:
            raise Exception(
                f"API Error: Unknown response code {response_code}"
            )

        if not data.get("results"):
            raise Exception("No questions returned from API")

        # å•é¡Œãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        question_data = data["results"][0]

        # URL ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        import urllib.parse

        question = urllib.parse.unquote(question_data["question"])
        correct_answer = urllib.parse.unquote(question_data["correct_answer"])
        incorrect_answers = [
            urllib.parse.unquote(ans)
            for ans in question_data["incorrect_answers"]
        ]

        # é¸æŠè‚¢ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        import random

        choices = [correct_answer] + incorrect_answers
        random.shuffle(choices)

        return ListeningProblem(
            question=question,
            choices=choices,
            correct_answer=correct_answer,
            difficulty=question_data["difficulty"],
            category=question_data["category"],
            explanation="",  # Trivia APIã«ã¯è§£èª¬ãŒãªã„ãŸã‚ç©ºæ–‡å­—
        )

    except Exception as e:
        print(f"Error fetching listening problem: {str(e)}")

        # å……å®Ÿã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å•é¡Œã‚»ãƒƒãƒˆ
        fallback_problems = [
            {
                "question": "What is the capital of Japan?",
                "choices": ["Tokyo", "Osaka", "Kyoto", "Hiroshima"],
                "correct_answer": "Tokyo",
                "difficulty": "easy",
                "category": "Geography",
            },
            {
                "question": "Which planet is known as the Red Planet?",
                "choices": ["Venus", "Mars", "Jupiter", "Saturn"],
                "correct_answer": "Mars",
                "difficulty": "easy",
                "category": "Science",
            },
            {
                "question": "How many continents are there on Earth?",
                "choices": ["5", "6", "7", "8"],
                "correct_answer": "7",
                "difficulty": "medium",
                "category": "Geography",
            },
            {
                "question": "What is the largest mammal in the world?",
                "choices": [
                    "Elephant",
                    "Blue Whale",
                    "Giraffe",
                    "Hippopotamus",
                ],
                "correct_answer": "Blue Whale",
                "difficulty": "medium",
                "category": "Science",
            },
        ]

        # é›£æ˜“åº¦ã«å¿œã˜ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å•é¡Œã‚’é¸æŠ
        suitable_problems = [
            p for p in fallback_problems if p["difficulty"] == difficulty
        ]
        if not suitable_problems:
            suitable_problems = (
                fallback_problems  # é©åˆ‡ãªé›£æ˜“åº¦ãŒãªã„å ´åˆã¯å…¨ã¦
            )

        import random

        selected_problem = random.choice(suitable_problems)

        return ListeningProblem(
            question=selected_problem["question"],
            choices=selected_problem["choices"],
            correct_answer=selected_problem["correct_answer"],
            difficulty=selected_problem["difficulty"],
            category=selected_problem["category"],
            explanation="This is a fallback question due to external API issues.",
        )


# ============================================================================
# ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œå›ç­”ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================================


@app.post("/api/listening/check", response_model=ListeningAnswerResponse)
async def check_listening_answer(req: ListeningAnswerRequest):
    """
    ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Args:
        req: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ãƒ‡ãƒ¼ã‚¿

    Returns:
        ListeningAnswerResponse: æ­£è§£åˆ¤å®šã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€è§£èª¬
    """
    try:
        # æ­£è§£åˆ¤å®šï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ï¼‰
        is_correct = (
            req.user_answer.strip().lower()
            == req.correct_answer.strip().lower()
        )

        # AIã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆ
        if model:
            prompt = f"""
ã‚ãªãŸã¯è‹±èªå­¦ç¿’è€…å‘ã‘ã®ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œãƒãƒ¥ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒªã‚¹ãƒ‹ãƒ³ã‚°å•é¡Œã®å›ç­”ã«ã¤ã„ã¦ã€åŠ±ã¾ã—ã¨ã¨ã‚‚ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {req.question}
é¸æŠè‚¢: {', '.join(req.choices)}
æ­£è§£: {req.correct_answer}
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”: {req.user_answer}
æ­£è§£åˆ¤å®š: {'æ­£è§£' if is_correct else 'ä¸æ­£è§£'}

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ä»¥ä¸‹ã®è¦ç´ ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
1. æ­£è§£ãƒ»ä¸æ­£è§£ã®åˆ¤å®š
2. æ­£è§£ã®ç†ç”±ã‚„èƒŒæ™¯çŸ¥è­˜
3. å­¦ç¿’è€…ã¸ã®åŠ±ã¾ã—ã®è¨€è‘‰
4. æ—¥æœ¬èªã§100æ–‡å­—ä»¥å†…

å›ç­”å½¢å¼ï¼šJSON
{{
    "feedback": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ–‡",
    "explanation": "è§£èª¬æ–‡"
}}
"""

            try:
                ai_response = model.generate_content(prompt)
                if ai_response.text:
                    import json
                    import re

                    # JSONã‚’æŠ½å‡º
                    json_match = re.search(
                        r"\{.*\}", ai_response.text, re.DOTALL
                    )
                    if json_match:
                        response_data = json.loads(json_match.group())
                        feedback = response_data.get("feedback", "")
                        explanation = response_data.get("explanation", "")
                    else:
                        raise Exception("No JSON found in AI response")
                else:
                    raise Exception("Empty AI response")

            except Exception as e:
                print(f"AI feedback generation failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
                if is_correct:
                    feedback = "æ­£è§£ã§ã™ï¼ã‚ˆãã§ãã¾ã—ãŸã€‚"
                    explanation = f"ç­”ãˆã¯ã€Œ{req.correct_answer}ã€ã§ã™ã€‚"
                else:
                    feedback = (
                        f"æƒœã—ã„ï¼æ­£è§£ã¯ã€Œ{req.correct_answer}ã€ã§ã—ãŸã€‚"
                    )
                    explanation = "æ¬¡å›ã‚‚é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼"
        else:
            # Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            if is_correct:
                feedback = "æ­£è§£ã§ã™ï¼ç´ æ™´ã‚‰ã—ã„ï¼"
                explanation = f"ç­”ãˆã¯ã€Œ{req.correct_answer}ã€ã§ã™ã€‚"
            else:
                feedback = (
                    f"ä¸æ­£è§£ã§ã™ã€‚æ­£è§£ã¯ã€Œ{req.correct_answer}ã€ã§ã—ãŸã€‚"
                )
                explanation = "æ¬¡å›ã‚‚é ‘å¼µã£ã¦ãã ã•ã„ï¼"

        return ListeningAnswerResponse(
            is_correct=is_correct, feedback=feedback, explanation=explanation
        )

    except Exception as e:
        print(f"Error checking listening answer: {str(e)}")
        return ListeningAnswerResponse(
            is_correct=False,
            feedback="å›ç­”ã®ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            explanation="ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        )


@app.post(
    "/api/instant-translation/check",
    response_model=InstantTranslationCheckResponse,
)
async def check_instant_translation_answer(
    req: InstantTranslationCheckRequest,
):
    """
    ç¬é–“è‹±ä½œæ–‡ã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’æ­£è§£ã¨æ¯”è¼ƒã—ã€AIã‚’ä½¿ã£ã¦è©³ç´°ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    print(f"ğŸ”” Instant translation check request: '{req.userAnswer[:30]}...'")

    try:
        if not model:
            # Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ¯”è¼ƒ
            is_correct = (
                req.userAnswer.lower().strip()
                == req.correctAnswer.lower().strip()
            )
            return InstantTranslationCheckResponse(
                isCorrect=is_correct,
                feedback=(
                    "Good try! Keep practicing."
                    if is_correct
                    else "Close, but not quite right. Try again!"
                ),
                score=100 if is_correct else 50,
                suggestions=[],
            )

        # AIã‚’ä½¿ã£ã¦è©³ç´°ãªå›ç­”ãƒã‚§ãƒƒã‚¯ï¼ˆéåŒæœŸå®Ÿè¡Œï¼‰
        check_prompt = create_translation_check_prompt(
            req.japanese, req.correctAnswer, req.userAnswer
        )

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(check_prompt)
        )

        if response.text:
            # AIå¿œç­”ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            ai_feedback = response.text

            # ç°¡å˜ãªæ­£è§£åˆ¤å®šï¼ˆAIã®å¿œç­”ã«åŸºã¥ãï¼‰
            is_correct = any(
                word in ai_feedback.lower()
                for word in ["correct", "good", "excellent", "right"]
            )

            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
            score = 100 if is_correct else 70

            return InstantTranslationCheckResponse(
                isCorrect=is_correct,
                feedback=ai_feedback,
                score=score,
                suggestions=[],
            )
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return InstantTranslationCheckResponse(
                isCorrect=False,
                feedback="Sorry, I couldn't evaluate your answer properly. Please try again.",
                score=50,
                suggestions=[],
            )

    except Exception as e:
        print(f"Error checking instant translation answer: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="Failed to check instant translation answer",
        )


def create_translation_check_prompt(
    japanese: str, correct_answer: str, user_answer: str
) -> str:
    """
    ç¬é–“è‹±ä½œæ–‡ã®å›ç­”ãƒã‚§ãƒƒã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ

    Args:
        japanese: æ—¥æœ¬èªã®åŸæ–‡
        correct_answer: æ­£è§£ã®è‹±èª
        user_answer: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”

    Returns:
        AIãŒå›ç­”ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """

    prompt = f"""
ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªè‹±èªæ•™å¸«ã§ã™ã€‚æ—¥æœ¬äººå­¦ç¿’è€…ã®ç¬é–“è‹±ä½œæ–‡ã®å›ç­”ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€å•é¡Œã€‘
æ—¥æœ¬èª: "{japanese}"
æ­£è§£: "{correct_answer}"
å­¦ç¿’è€…ã®å›ç­”: "{user_answer}"

ã€è©•ä¾¡åŸºæº–ã€‘
- æ„å‘³ãŒæ­£ç¢ºã«ä¼ã‚ã£ã¦ã„ã‚‹ã‹
- æ–‡æ³•ãŒæ­£ã—ã„ã‹
- è‡ªç„¶ãªè‹±èªè¡¨ç¾ã‹
- èªå½™ã®é¸æŠãŒé©åˆ‡ã‹

ã€è¿”ç­”å½¢å¼ã€‘
ä»¥ä¸‹ã®å½¢å¼ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
- ã€ŒExcellent!ã€ã€ŒGood!ã€ã€ŒNot quite rightã€ã®ã„ãšã‚Œã‹ã§å§‹ã‚ã‚‹
- å…·ä½“çš„ãªæ”¹å–„ç‚¹ã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’å«ã‚ã‚‹
- åŠ±ã¾ã—ã®è¨€è‘‰ã‚’å«ã‚ã‚‹
- 2-3æ–‡ã§ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹

æ—¥æœ¬äººå­¦ç¿’è€…ã«ã¨ã£ã¦ç†è§£ã—ã‚„ã™ãã€å­¦ç¿’æ„æ¬²ã‚’é«˜ã‚ã‚‹ã‚ˆã†ãªè©•ä¾¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""

    return prompt


@app.get(
    "/api/eiken-translation-problem",
    response_model=InstantTranslationProblem,
)
async def get_eiken_translation_problem(
    difficulty: str = "all", category: str = "all", eiken_level: str = ""
):
    """
    è‹±æ¤œå¯¾å¿œç¬é–“è‹±ä½œæ–‡å•é¡Œå–å¾—APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    /api/instant-translation/problemã¨åŒã˜æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

    Args:
        difficulty: å•é¡Œã®é›£æ˜“åº¦ (all, basic, intermediate, advanced)
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (all, daily_life, work, travel, etc.)
        eiken_level: è‹±æ¤œãƒ¬ãƒ™ãƒ« (5, 4, 3, pre-2, 2, pre-1, 1)
    """

    # æ—¢å­˜ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹
    return await get_instant_translation_problem(
        difficulty, category, eiken_level, False
    )


def create_eiken_problem_generation_prompt(
    eiken_level: str, category: str = "general", long_text_mode: bool = False
) -> str:
    """
    è‹±æ¤œãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸç¬é–“è‹±ä½œæ–‡å•é¡Œã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ

    Args:
        eiken_level: è‹±æ¤œãƒ¬ãƒ™ãƒ« (5, 4, 3, pre-2, 2, pre-1, 1)
        category: å•é¡Œã®ã‚«ãƒ†ã‚´ãƒª (daily_life, work, travel, etc.)

    Returns:
        AIãŒå•é¡Œã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """

    # è‹±æ¤œãƒ¬ãƒ™ãƒ«åˆ¥ã®ç‰¹å¾´å®šç¾©
    eiken_characteristics = {
        "5": {
            "description": "è‹±æ¤œ5ç´š (ä¸­å­¦åˆç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "ç¾åœ¨å½¢ã€éå»å½¢ã€beå‹•è©ã€ä¸€èˆ¬å‹•è©ã®åŸºæœ¬å½¢",
            "vocabulary": "ä¸­å­¦1å¹´ç”Ÿãƒ¬ãƒ™ãƒ«ã®åŸºæœ¬èªå½™ (ç´„600èª)",
            "sentence_structure": "ã‚·ãƒ³ãƒ—ãƒ«ãªå˜æ–‡ä¸­å¿ƒ",
            "examples": [
                "I am a student.",
                "I go to school.",
                "It is sunny today.",
            ],
        },
        "4": {
            "description": "è‹±æ¤œ4ç´š (ä¸­å­¦ä¸­ç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "åŠ©å‹•è© (can, will, must)ã€æœªæ¥å½¢ã€é€²è¡Œå½¢",
            "vocabulary": "ä¸­å­¦2å¹´ç”Ÿãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„1300èª)",
            "sentence_structure": "åŠ©å‹•è©ã‚’å«ã‚€æ–‡ã€ç–‘å•æ–‡ãƒ»å¦å®šæ–‡",
            "examples": [
                "I can play tennis.",
                "Will you help me?",
                "She is reading a book.",
            ],
        },
        "3": {
            "description": "è‹±æ¤œ3ç´š (ä¸­å­¦å’æ¥­ãƒ¬ãƒ™ãƒ«)",
            "grammar": "å—å‹•æ…‹ã€ç¾åœ¨å®Œäº†ã€ä¸å®šè©ã€å‹•åè©",
            "vocabulary": "ä¸­å­¦3å¹´ç”Ÿãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„2100èª)",
            "sentence_structure": "è¤‡æ–‡æ§‹é€ ã€æ¥ç¶šè©ã‚’ä½¿ã£ãŸæ–‡",
            "examples": [
                "This book was written by him.",
                "I have been to Tokyo.",
                "I want to learn English.",
            ],
        },
        "pre-2": {
            "description": "è‹±æ¤œæº–2ç´š (é«˜æ ¡ä¸­ç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "é–¢ä¿‚ä»£åè©ã€ä»®å®šæ³•ã®åŸºæœ¬ã€åˆ†è©",
            "vocabulary": "é«˜æ ¡åŸºç¤ãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„3600èª)",
            "sentence_structure": "é–¢ä¿‚è©ã‚’ä½¿ã£ãŸè¤‡æ–‡ã€ã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ ",
            "examples": [
                "The man who is standing there is my teacher.",
                "If I were you, I would study harder.",
            ],
        },
        "2": {
            "description": "è‹±æ¤œ2ç´š (é«˜æ ¡å’æ¥­ãƒ¬ãƒ™ãƒ«)",
            "grammar": "ä»®å®šæ³•ã€è¤‡é›‘ãªæ™‚åˆ¶ã€é«˜åº¦ãªæ–‡å‹",
            "vocabulary": "é«˜æ ¡å’æ¥­ãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„5100èª)",
            "sentence_structure": "è¤‡é›‘ãªè¤‡æ–‡ã€è«–ç†çš„ãªæ–‡æ§‹é€ ",
            "examples": [
                "If I had studied harder, I could have passed the exam.",
                "Having finished my homework, I went to bed.",
            ],
        },
        "pre-1": {
            "description": "è‹±æ¤œæº–1ç´š (å¤§å­¦ä¸­ç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "é«˜åº¦ãªæ–‡æ³•æ§‹é€ ã€è«–ç†çš„è¡¨ç¾",
            "vocabulary": "å¤§å­¦ä¸­ç´šãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„7500èª)",
            "sentence_structure": "å­¦è¡“çš„ãƒ»ãƒ“ã‚¸ãƒã‚¹çš„è¡¨ç¾",
            "examples": [
                "The proposal is likely to be implemented next year.",
                "It is essential that we address this issue promptly.",
            ],
        },
        "1": {
            "description": "è‹±æ¤œ1ç´š (å¤§å­¦ä¸Šç´šãƒ¬ãƒ™ãƒ«)",
            "grammar": "æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®æ–‡æ³•ã€æ…£ç”¨è¡¨ç¾",
            "vocabulary": "å¤§å­¦ä¸Šç´šãƒ¬ãƒ™ãƒ«ã®èªå½™ (ç´„10000-15000èª)",
            "sentence_structure": "é«˜åº¦ãªè«–ç†æ§‹é€ ã€å°‚é–€çš„è¡¨ç¾",
            "examples": [
                "The ramifications of this decision could be far-reaching.",
                "Notwithstanding the challenges, we must persevere.",
            ],
        },
    }

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯
    category_topics = {
        "daily_life": ["å®¶æ—", "é£Ÿäº‹", "è²·ã„ç‰©", "è¶£å‘³", "å¤©æ°—"],
        "work": ["ä»•äº‹", "ä¼šè­°", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", "åŒåƒš", "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"],
        "travel": ["æ—…è¡Œ", "äº¤é€š", "å®¿æ³Š", "è¦³å…‰", "æ–‡åŒ–"],
        "education": ["å­¦æ ¡", "å‹‰å¼·", "è©¦é¨“", "å›³æ›¸é¤¨", "æˆæ¥­"],
        "health": ["å¥åº·", "ç—…æ°—", "é‹å‹•", "é£Ÿäº‹", "ç—…é™¢"],
        "technology": [
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼",
            "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³",
            "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ",
            "ã‚¢ãƒ—ãƒª",
            "SNS",
        ],
        "general": ["ä¸€èˆ¬çš„ãªè©±é¡Œ", "æ—¥å¸¸çš„ãªè¡¨ç¾", "åŸºæœ¬çš„ãªä¼šè©±"],
    }

    eiken_info = eiken_characteristics.get(
        eiken_level, eiken_characteristics["3"]
    )
    topics = category_topics.get(category, category_topics["general"])

    prompt = f"""
ã‚ãªãŸã¯è‹±æ¤œå¯¾ç­–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æ¡ä»¶ã«å¾“ã£ã¦ç¬é–“è‹±ä½œæ–‡ã®å•é¡Œã‚’1ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ãƒ¬ãƒ™ãƒ«ã€‘
{eiken_info['description']}

ã€æ–‡æ³•ãƒ¬ãƒ™ãƒ«ã€‘
{eiken_info['grammar']}

ã€èªå½™ãƒ¬ãƒ™ãƒ«ã€‘
{eiken_info['vocabulary']}

ã€æ–‡æ§‹é€ ã€‘
{eiken_info['sentence_structure']}

ã€å‚è€ƒä¾‹æ–‡ã€‘
{', '.join(eiken_info['examples'])}

ã€å•é¡Œã‚«ãƒ†ã‚´ãƒªã€‘
{category} - ãƒˆãƒ”ãƒƒã‚¯ä¾‹: {', '.join(topics)}

ã€ä½œæˆæ¡ä»¶ã€‘
1. æŒ‡å®šã•ã‚ŒãŸè‹±æ¤œãƒ¬ãƒ™ãƒ«ã«é©ã—ãŸèªå½™ãƒ»æ–‡æ³•ã®ã¿ã‚’ä½¿ç”¨
2. æ—¥æœ¬äººå­¦ç¿’è€…ã«ã¨ã£ã¦å®Ÿç”¨æ€§ã®é«˜ã„è¡¨ç¾
3. è‡ªç„¶ã§é©åˆ‡ãªè‹±èªè¡¨ç¾
4. æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«é–¢é€£ã™ã‚‹å†…å®¹
5. {"è¤‡æ•°æ–‡ã§æ§‹æˆã™ã‚‹ï¼ˆé•·æ–‡ãƒ¢ãƒ¼ãƒ‰ï¼‰" if long_text_mode else "çŸ­ã„1æ–‡ã®ã¿ã§æ§‹æˆã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
    "japanese": "æ—¥æœ¬èªã®æ–‡ç« ",
    "english": "å¯¾å¿œã™ã‚‹è‹±èªã®æ–‡ç« ",
    "difficulty": "easy/medium/hard",
    "category": "ã‚«ãƒ†ã‚´ãƒªå"
}}

{"2-3æ–‡ã‹ã‚‰ãªã‚‹" if long_text_mode else "1ã¤ã®"}å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""

    return prompt
