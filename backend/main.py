"""
FastAPI backend for the English Communication App.

This backend server provides AI-powered English conversation practice
for Japanese learners using Google Gemini AI and Google Cloud TTS.

Key features:
- AI conversation responses using Gemini
- Text-to-speech synthesis for pronunciation practice
- Grammar checking and feedback
- Voice input and output customization
"""

import asyncio
import base64
import os
from concurrent.futures import ThreadPoolExecutor

import google.generativeai as genai
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel

# Load environment variables from the `.env` file located at the project root.
# This allows developers to keep API keys outside of the source code for security.
# The load_dotenv() function automatically reads the .env file from the project root.
load_dotenv()

# API keys and credentials read from environment variables
# These are set in the .env file and loaded at runtime
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv(
    "GOOGLE_APPLICATION_CREDENTIALS", ""
)

# Configure Gemini AI model for conversation generation
# The gemini-2.5-flash model provides fast, high-quality responses
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel("gemini-2.5-flash")
    # Initialize Gemini TTS model
    tts_model = genai.GenerativeModel("gemini-2.5-flash-preview-tts")
else:
    model = None
    tts_model = None

# „Çπ„É¨„ÉÉ„Éâ„Éó„Éº„É´„Ç®„Ç∞„Çº„Ç≠„É•„Éº„Çø„Éº„ÇíË®≠ÂÆöÔºàAIÂá¶ÁêÜ„ÇíÈùûÂêåÊúüÂåñ„Åô„Çã„Åü„ÇÅÔºâ
executor = ThreadPoolExecutor(max_workers=4)

# „É°„É¢„É™„Ç≠„É£„ÉÉ„Ç∑„É•ÔºàÂçò‰∏Ä„É¶„Éº„Ç∂„ÉºÁî®„ÅÆÈ´òÈÄüÂåñÔºâ
response_cache = {}
CACHE_TTL = 300  # 5ÂàÜÈñì„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•

# Create FastAPI application instance
app = FastAPI()

# Add CORS middleware to allow frontend connections from React development server
# This is necessary for the frontend (localhost:3000) to communicate with backend (localhost:8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods (GET, POST, etc.)
    allow_headers=["*"],  # Allow all headers
)
# Pydantic models define the structure of requests and responses
# These ensure type safety and automatic validation


class Request(BaseModel):
    """
    Request model for conversation API calls from the frontend.

    This defines what data the frontend must send when requesting
    an AI response for English conversation practice.
    """

    text: str  # The user's input text or speech transcription
    conversation_history: list = []  # Previous messages for context
    enable_grammar_check: bool = True  # Whether to enable grammar checking


class JapaneseConsultationRequest(BaseModel):
    """
    Request model for Japanese consultation and dictionary features.
    
    For asking questions about English expressions, grammar, or vocabulary
    with responses in Japanese.
    """
    
    text: str  # User's question in Japanese or English
    conversation_history: list = []  # Previous consultation messages


class Response(BaseModel):
    """
    Response model returned by the API to the frontend.

    Contains the AI's response text that will be displayed
    and potentially converted to speech.
    """

    reply: str  # The AI's response text


class TTSRequest(BaseModel):
    """
    Request model for text-to-speech synthesis.

    Defines parameters for converting text to natural-sounding speech
    using Gemini 2.5 Flash Preview TTS service.
    """

    text: str  # Text to convert to speech
    voice_name: str = (
        "Kore"  # Default: bright female English voice for Gemini TTS
    )
    language_code: str = "en-US"  # Language and region code
    speaking_rate: float = 1.0  # Speech speed (0.25-4.0, 1.0 = normal)


class InstantTranslationCheckRequest(BaseModel):
    """
    Request model for instant translation answer checking.

    Defines the structure for checking user answers against correct translations
    in the instant translation mode.
    """

    japanese: str  # Original Japanese text
    correctAnswer: str  # Correct English translation
    userAnswer: str  # User's English translation attempt


class InstantTranslationProblem(BaseModel):
    """
    Response model for instant translation problems.

    Contains a Japanese sentence to be translated to English.
    """

    japanese: str  # Japanese sentence to translate
    english: str  # Correct English translation
    difficulty: str = "medium"  # Problem difficulty level
    category: str = "general"  # Grammar or topic category


class InstantTranslationCheckResponse(BaseModel):
    """
    Response model for instant translation answer checking.

    Contains evaluation results and feedback for the user's translation attempt.
    """

    isCorrect: bool  # Whether the answer is correct
    feedback: str  # Detailed feedback on the translation
    score: int  # Numerical score (0-100)
    suggestions: list = []  # Alternative translations or improvements


class CombinedResponse(BaseModel):
    """
    Combined response model for simultaneous text and audio generation.

    Contains both AI text response and TTS audio data for optimized performance.
    """

    reply: str  # The AI's text response
    audio_data: str = ""  # Base64 encoded audio data
    content_type: str = "text/plain"  # Audio MIME type
    use_browser_tts: bool = False  # Whether to fallback to browser TTS
    fallback_text: str = ""  # Text for browser TTS fallback
    processing_time: float = 0.0  # Total processing time in seconds


# API Endpoints
# These endpoints handle communication between the frontend and backend

# ÂøÖË¶Å„Å™„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÇíËøΩÂä†
import time


@app.get("/")
async def root():
    """
    Health check endpoint - confirms the API server is running.

    This is useful for monitoring and debugging server status.
    Returns a simple JSON message when the server is operational.
    """
    return {"message": "English Communication App API is running"}


@app.get("/health")
async def health_check():
    """
    Health check endpoint for Docker container monitoring.

    This endpoint is used by Docker's HEALTHCHECK instruction
    to verify that the application is running properly.

    Returns:
        dict: Simple status message indicating the service is healthy
    """
    return {"status": "healthy", "service": "eikaiwa-backend"}


@app.get("/api/status")
async def api_status():
    """
    Configuration status endpoint - checks if required services are available.

    Returns the status of:
    - Gemini AI API (for conversation generation)
    - Google Cloud credentials (for authentication)
    - TTS service (for speech synthesis)

    This helps users troubleshoot configuration issues.
    """
    return {
        "gemini_configured": bool(GEMINI_API_KEY and model),
        "google_credentials_configured": bool(
            GOOGLE_APPLICATION_CREDENTIALS
            and os.path.exists(GOOGLE_APPLICATION_CREDENTIALS)
        ),
        "gemini_tts_configured": bool(GEMINI_API_KEY and tts_model),
        "tts_configured": bool(tts_model),
    }


@app.post("/api/tts")
async def text_to_speech(request: TTSRequest):
    """Convert text to speech using Gemini TTS."""

    if not tts_model:
        raise HTTPException(
            status_code=503, detail="TTS service not available"
        )

    try:
        # TTS„Ç≠„É£„ÉÉ„Ç∑„É•„ÉÅ„Çß„ÉÉ„ÇØ
        import time

        tts_cache_key = f"tts_{hash(request.text)}_{request.voice_name}_{request.speaking_rate}"
        if tts_cache_key in response_cache:
            cached_data, timestamp = response_cache[tts_cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"‚úÖ TTS Cache hit for: {request.text[:30]}...")
                return cached_data

        # Gemini 2.5 Flash Preview TTS with dictionary-based config
        content = request.text

        # Configure generation with dictionary format
        generation_config = {
            "response_modalities": ["AUDIO"],
            "speech_config": {
                "voice_config": {
                    "prebuilt_voice_config": {"voice_name": request.voice_name}
                }
            },
        }

        # Generate audio using Gemini TTS model (ÈùûÂêåÊúüÂÆüË°å)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor,
            lambda: tts_model.generate_content(
                contents=content, generation_config=generation_config
            ),
        )

        # Extract audio data from response
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                for part in candidate.content.parts:
                    if hasattr(part, "inline_data") and part.inline_data:
                        # Found audio data - extract properly
                        audio_data = part.inline_data.data
                        mime_type = part.inline_data.mime_type or "audio/wav"

                        print(
                            f"üéµ Audio data found: type={type(audio_data)}, mime_type={mime_type}"
                        )

                        # Handle different data types from Gemini
                        if isinstance(audio_data, bytes):
                            # If bytes, encode to base64
                            audio_base64 = base64.b64encode(audio_data).decode(
                                "utf-8"
                            )
                            print(
                                f"üîÑ Encoded bytes to base64: {len(audio_data)} bytes -> {len(audio_base64)} chars"
                            )
                        elif isinstance(audio_data, str):
                            # If already string, assume it's base64
                            audio_base64 = audio_data
                            print(
                                f"üîÑ Using string as base64: {len(audio_base64)} chars"
                            )
                        else:
                            print(
                                f"‚ùå Unexpected audio data type: {type(audio_data)}"
                            )
                            raise ValueError(
                                f"Unexpected audio data type: {type(audio_data)}"
                            )

                        # Validate base64 data
                        try:
                            # Test decode to verify it's valid base64
                            decoded_test = base64.b64decode(audio_base64)
                            print(
                                f"‚úÖ Base64 validation successful: {len(decoded_test)} bytes decoded"
                            )
                        except Exception as b64_error:
                            print(f"‚ùå Base64 validation failed: {b64_error}")
                            raise ValueError(
                                f"Invalid base64 audio data: {b64_error}"
                            )

                        result = {
                            "audio_data": audio_base64,
                            "content_type": mime_type,
                            "original_size": (
                                len(audio_data)
                                if isinstance(audio_data, (bytes, str))
                                else 0
                            ),
                        }
                        # TTS„É¨„Çπ„Éù„É≥„Çπ„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠ò
                        response_cache[tts_cache_key] = (result, time.time())
                        return result

        # If no audio data found, fallback to browser TTS
        print("No audio data found in Gemini TTS response")
        fallback_result = {
            "audio_data": "",
            "content_type": "text/plain",
            "fallback_text": request.text,
            "use_browser_tts": True,
        }
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÁµêÊûú„ÇÇ„Ç≠„É£„ÉÉ„Ç∑„É•
        response_cache[tts_cache_key] = (fallback_result, time.time())
        return fallback_result

    except HTTPException:
        # Propagate HTTP errors such as 503 without modification
        raise
    except Exception as e:
        print(f"Gemini TTS Error: {str(e)}")
        # Fallback to browser TTS
        error_result = {
            "audio_data": "",
            "content_type": "text/plain",
            "fallback_text": request.text,
            "use_browser_tts": True,
            "error": str(e),
        }
        # „Ç®„É©„ÉºÁµêÊûú„ÅØÁü≠ÊôÇÈñì„Ç≠„É£„ÉÉ„Ç∑„É•Ôºà30ÁßíÔºâ
        response_cache[tts_cache_key] = (
            error_result,
            time.time() - CACHE_TTL + 30,
        )
        return error_result


@app.get("/api/welcome", response_model=Response)
async def get_welcome_message():
    """Generate a personalized welcome message."""

    print("üîî Welcome request received")

    try:
        if not model:
            return Response(
                reply="Hello! Welcome to English Communication App! Please set up your API key to get started."
            )

        welcome_prompt = create_welcome_prompt()
        # AIÁîüÊàê„ÇíÈùûÂêåÊúüÂÆüË°å
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(welcome_prompt)
        )

        if response.text:
            return Response(reply=response.text)
        else:
            return Response(
                reply="Hello! Welcome to English Communication App! Let's start practicing English together!"
            )

    except Exception as e:
        print(f"Error generating welcome message: {str(e)}")
        return Response(
            reply="Hello! Welcome to English Communication App! I'm here to help you practice English. How are you today?"
        )


@app.post("/api/respond", response_model=Response)
async def respond(req: Request):
    """Generate a response using Gemini API for English conversation practice."""

    print(f"üîî Response request received: text='{req.text[:50]}...'")

    try:
        if not model:
            # Fallback response if Gemini API is not configured
            return Response(
                reply="API key not configured. Please set GEMINI_API_KEY environment variable."
            )

        # „Ç≠„É£„ÉÉ„Ç∑„É•„ÉÅ„Çß„ÉÉ„ÇØ
        cache_key = (
            f"response_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )
        import time

        if cache_key in response_cache:
            cached_data, timestamp = response_cache[cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"‚úÖ Cache hit for response: {req.text[:30]}...")
                return Response(reply=cached_data)

        # Create conversation prompt
        prompt = create_conversation_prompt(req.text, req.conversation_history)

        # Generate response using Gemini (ÈùûÂêåÊúüÂÆüË°å)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        if response.text:
            # „É¨„Çπ„Éù„É≥„Çπ„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠ò
            response_cache[cache_key] = (response.text, time.time())
            return Response(reply=response.text)
        else:
            return Response(
                reply="Sorry, I couldn't generate a response. Please try again."
            )

    except Exception as e:
        # Log the error in production, but don't expose internal details
        print(f"Error generating response: {str(e)}")
        return Response(
            reply="Sorry, there was an error processing your request. Please try again."
        )


@app.post("/api/japanese-consultation", response_model=Response)
async def japanese_consultation(req: JapaneseConsultationRequest):
    """Generate Japanese consultation response for English expression and grammar questions."""

    print(f"üîî Japanese consultation request: text='{req.text[:50]}...'")

    try:
        if not model:
            # Fallback response if Gemini API is not configured
            return Response(
                reply="Áî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅAPI„Ç≠„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇGEMINI_API_KEY„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
            )

        # „Ç≠„É£„ÉÉ„Ç∑„É•„ÉÅ„Çß„ÉÉ„ÇØ
        cache_key = (
            f"consultation_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )
        import time

        if cache_key in response_cache:
            cached_data, timestamp = response_cache[cache_key]
            if time.time() - timestamp < CACHE_TTL:
                print(f"‚úÖ Cache hit for consultation: {req.text[:30]}...")
                return Response(reply=cached_data)

        # Create Japanese consultation prompt
        prompt = create_japanese_consultation_prompt(
            req.text, "general", req.conversation_history
        )

        # Generate response using Gemini (ÈùûÂêåÊúüÂÆüË°å)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        if response.text:
            # „É¨„Çπ„Éù„É≥„Çπ„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠ò
            response_cache[cache_key] = (response.text, time.time())
            return Response(reply=response.text)
        else:
            return Response(
                reply="Áî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅÂõûÁ≠î„ÇíÁîüÊàê„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ„ÇÇ„ÅÜ‰∏ÄÂ∫¶„ÅäË©¶„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ"
            )

    except Exception as e:
        # Log the error in production, but don't expose internal details
        print(f"Error generating Japanese consultation response: {str(e)}")
        return Response(
            reply="Áî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅ„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ„ÇÇ„ÅÜ‰∏ÄÂ∫¶„ÅäË©¶„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ"
        )


@app.post("/api/respond-with-audio", response_model=CombinedResponse)
async def respond_with_audio(
    req: Request, voice_name: str = "Kore", speaking_rate: float = 1.0
):
    """
    Generate both text response and audio simultaneously for optimal performance.

    This endpoint combines conversation generation and TTS processing
    to reduce total response time for single-user scenarios.
    """

    print(
        f"üîî Combined response request: text='{req.text[:50]}...', voice={voice_name}"
    )
    start_time = time.time()

    try:
        if not model:
            return CombinedResponse(
                reply="API key not configured. Please set GEMINI_API_KEY environment variable.",
                use_browser_tts=True,
                fallback_text="API key not configured. Please set GEMINI_API_KEY environment variable.",
            )

        # „Ç≠„É£„ÉÉ„Ç∑„É•„ÉÅ„Çß„ÉÉ„ÇØ
        import time

        cache_key = (
            f"response_{hash(req.text)}_{hash(str(req.conversation_history))}"
        )

        # „ÉÜ„Ç≠„Çπ„Éà„É¨„Çπ„Éù„É≥„Çπ„ÇíÁîüÊàê
        prompt = create_conversation_prompt(req.text, req.conversation_history)
        loop = asyncio.get_event_loop()

        # AI„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê„ÇíÈùûÂêåÊúüÂÆüË°å
        response_future = loop.run_in_executor(
            executor, lambda: model.generate_content(prompt)
        )

        # AI„É¨„Çπ„Éù„É≥„Çπ„ÇíÂæÖ„Å§
        ai_response = await response_future

        if not ai_response.text:
            return CombinedResponse(
                reply="Sorry, I couldn't generate a response. Please try again.",
                use_browser_tts=True,
                fallback_text="Sorry, I couldn't generate a response. Please try again.",
            )

        reply_text = ai_response.text

        # TTSÁîüÊàê„Çí‰∏¶ÂàóÂÆüË°åÔºàAI„É¨„Çπ„Éù„É≥„ÇπÂæåÔºâ
        if tts_model:
            tts_cache_key = (
                f"tts_{hash(reply_text)}_{voice_name}_{speaking_rate}"
            )

            # TTS„Ç≠„É£„ÉÉ„Ç∑„É•„ÉÅ„Çß„ÉÉ„ÇØ
            if tts_cache_key in response_cache:
                cached_tts, timestamp = response_cache[tts_cache_key]
                if time.time() - timestamp < CACHE_TTL:
                    print(f"‚úÖ TTS Cache hit for combined response")
                    processing_time = time.time() - start_time
                    return CombinedResponse(
                        reply=reply_text,
                        audio_data=cached_tts.get("audio_data", ""),
                        content_type=cached_tts.get(
                            "content_type", "text/plain"
                        ),
                        use_browser_tts=cached_tts.get(
                            "use_browser_tts", False
                        ),
                        fallback_text=cached_tts.get(
                            "fallback_text", reply_text
                        ),
                        processing_time=processing_time,
                    )

            # TTSÁîüÊàêË®≠ÂÆö
            generation_config = {
                "response_modalities": ["AUDIO"],
                "speech_config": {
                    "voice_config": {
                        "prebuilt_voice_config": {"voice_name": voice_name}
                    }
                },
            }

            try:
                # TTSÁîüÊàê„ÇíÈùûÂêåÊúüÂÆüË°å
                tts_response = await loop.run_in_executor(
                    executor,
                    lambda: tts_model.generate_content(
                        contents=reply_text,
                        generation_config=generation_config,
                    ),
                )

                # TTS„Ç™„Éº„Éá„Ç£„Ç™„Éá„Éº„Çø„ÇíÊäΩÂá∫
                audio_data = ""
                content_type = "text/plain"
                use_browser_tts = True

                if (
                    tts_response.candidates
                    and len(tts_response.candidates) > 0
                ):
                    candidate = tts_response.candidates[0]
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if (
                                hasattr(part, "inline_data")
                                and part.inline_data
                            ):
                                raw_audio = part.inline_data.data
                                content_type = (
                                    part.inline_data.mime_type or "audio/wav"
                                )

                                if isinstance(raw_audio, bytes):
                                    audio_data = base64.b64encode(
                                        raw_audio
                                    ).decode("utf-8")
                                elif isinstance(raw_audio, str):
                                    audio_data = raw_audio

                                if audio_data:
                                    use_browser_tts = False
                                    break

                # TTSÁµêÊûú„Çí„Ç≠„É£„ÉÉ„Ç∑„É•
                tts_result = {
                    "audio_data": audio_data,
                    "content_type": content_type,
                    "use_browser_tts": use_browser_tts,
                    "fallback_text": reply_text if use_browser_tts else "",
                }
                response_cache[tts_cache_key] = (tts_result, time.time())

                processing_time = time.time() - start_time
                print(
                    f"‚öôÔ∏è Combined processing completed in {processing_time:.2f}s"
                )

                return CombinedResponse(
                    reply=reply_text,
                    audio_data=audio_data,
                    content_type=content_type,
                    use_browser_tts=use_browser_tts,
                    fallback_text=reply_text if use_browser_tts else "",
                    processing_time=processing_time,
                )

            except Exception as tts_error:
                print(f"TTS Error in combined response: {str(tts_error)}")
                # TTS„Ç®„É©„ÉºÊôÇ„ÅØ„Éñ„É©„Ç¶„Ç∂TTS„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
                processing_time = time.time() - start_time
                return CombinedResponse(
                    reply=reply_text,
                    use_browser_tts=True,
                    fallback_text=reply_text,
                    processing_time=processing_time,
                )

        # TTS„É¢„Éá„É´„ÅåÁÑ°„ÅÑÂ†¥Âêà
        processing_time = time.time() - start_time
        return CombinedResponse(
            reply=reply_text,
            use_browser_tts=True,
            fallback_text=reply_text,
            processing_time=processing_time,
        )

    except Exception as e:
        print(f"Error in combined response: {str(e)}")
        processing_time = time.time() - start_time
        return CombinedResponse(
            reply="Sorry, there was an error processing your request. Please try again.",
            use_browser_tts=True,
            fallback_text="Sorry, there was an error processing your request. Please try again.",
            processing_time=processing_time,
        )


def create_conversation_prompt(
    user_text: str, conversation_history: list = None
) -> str:
    """
    Create prompts for English conversation practice.

    Args:
        user_text: The user's input message
        conversation_history: Previous messages for context

    Returns:
        A formatted prompt string optimized for conversation practice
    """

    # Format conversation history for context
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        history_context = "\n\nCONVERSATION HISTORY (for context):\n"
        # Show last 10 messages to avoid token limit issues
        recent_history = (
            conversation_history[-10:]
            if len(conversation_history) > 10
            else conversation_history
        )
        for msg in recent_history:
            sender = msg.get("sender", "Unknown")
            text = msg.get("text", "")
            history_context += f"{sender}: {text}\n"
        history_context += "\n"

    # Simplified conversation prompt
    prompt = f"""
You are an expert English teacher and conversation partner specializing in helping Japanese learners.

IMPORTANT GUIDELINES:
- Always be encouraging and supportive
- Use natural, conversational English
- Provide gentle corrections when needed
- Ask follow-up questions to keep the conversation flowing
- Use examples and explanations when helpful
- Reference previous parts of the conversation when relevant
- Keep responses concise and engaging (1-3 sentences)
- Focus on practical, everyday English
{history_context}

CURRENT MESSAGE FROM STUDENT:
"{user_text}"

Please respond naturally as a friendly English teacher and conversation partner.
"""

    return prompt


def create_welcome_prompt() -> str:
    """Create a welcome prompt for new users."""

    prompt = """
You are an expert English teacher and conversation partner specializing in helping Japanese learners.

Please create a warm, encouraging welcome message for a new student starting English conversation practice.

GUIDELINES:
- Keep it friendly and encouraging
- Mention that you're here to help with English conversation
- Invite them to start practicing by asking a question or sharing something about themselves
- Keep it concise (2-3 sentences)
- Use clear, natural English

Please respond with a welcoming message to get the conversation started.
"""

    return prompt


def create_japanese_consultation_prompt(
    user_text: str, consultation_type: str = "general", conversation_history: list = None
) -> str:
    """
    Create prompts for Japanese consultation about English expressions and grammar.

    Args:
        user_text: The user's question in Japanese or English
        consultation_type: Type of consultation (kept for API compatibility)
        conversation_history: Previous consultation messages for context

    Returns:
        A formatted prompt string optimized for Japanese consultation responses
    """

    # Format conversation history for context
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        history_context = "\n\nÁõ∏Ë´áÂ±•Ê≠¥ÔºàÂèÇËÄÉÊÉÖÂ†±Ôºâ:\n"
        # Show last 8 messages to avoid token limit issues
        recent_history = (
            conversation_history[-8:]
            if len(conversation_history) > 8
            else conversation_history
        )
        for msg in recent_history:
            sender = msg.get("sender", "Unknown")
            text = msg.get("text", "")
            history_context += f"{sender}: {text}\n"
        history_context += "\n"

    # Simple Japanese consultation prompt
    prompt = f"""
„ÅÇ„Å™„Åü„ÅØÊó•Êú¨‰∫∫„ÅÆËã±Ë™ûÂ≠¶ÁøíËÄÖ„ÇíÂ∞ÇÈñÄ„Å®„Åô„Çã„ÄÅÁµåÈ®ìË±äÂØå„ÅßË¶™Âàá„Å™Ëã±Ë™ûÊïôÂ∏´„Åß„Åô„ÄÇ

„ÄêÈáçË¶Å„Å™ÊåáÁ§∫„Äë:
- ÂøÖ„ÅöÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- Á∞°ÊΩî„ÅßÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑË™¨Êòé„ÇíÂøÉ„Åå„Åë„Å¶„Åè„Å†„Åï„ÅÑÔºà2-3ÊñáÁ®ãÂ∫¶Ôºâ
- 1„Å§„ÅÆÂÖ∑‰ΩìÁöÑ„Å™‰æãÊñá„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ
- ‰∏ÄÁõÆ„ÅßË™≠„ÇÅ„ÇãÁü≠„Åï„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- Ë¶ÅÁÇπ„Å†„Åë„ÇíÁ∞°ÊΩî„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ
{history_context}

„ÄêÂ≠¶ÁøíËÄÖ„Åã„Çâ„ÅÆË≥™Âïè„Äë:
"{user_text}"

‰∏äË®ò„ÅÆË≥™Âïè„Å´ÂØæ„Åó„Å¶„ÄÅÊó•Êú¨Ë™û„ÅßÁ∞°ÊΩî„Å´ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰æãÊñá„ÅØ1„Å§„Å†„Åë„ÄÅË™¨Êòé„ÅØ2-3Êñá‰ª•ÂÜÖ„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ
"""

    return prompt


def optimize_cache_cleanup():
    """
    „Ç≠„É£„ÉÉ„Ç∑„É•„ÅÆ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„ÇíÂÆüË°åÔºà„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÊúÄÈÅ©ÂåñÔºâ
    """
    import time

    current_time = time.time()
    expired_keys = []

    for key, (data, timestamp) in response_cache.items():
        if current_time - timestamp > CACHE_TTL:
            expired_keys.append(key)

    for key in expired_keys:
        del response_cache[key]

    print(f"üßπ Cache cleanup: removed {len(expired_keys)} expired entries")


# „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ëµ∑ÂãïÊôÇ„Å´„Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„ÇíÂÆöÊúüÂÆüË°å„Åô„Çã„Åü„ÇÅ„ÅÆ„Çø„Çπ„ÇØ
import threading
import time


def periodic_cache_cleanup():
    """ÂÆöÊúüÁöÑ„Å™„Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
    while True:
        time.sleep(300)  # 5ÂàÜÊØé„Å´ÂÆüË°å
        optimize_cache_cleanup()


# „Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„Éâ„Åß„Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„ÇíÈñãÂßã
cleanup_thread = threading.Thread(target=periodic_cache_cleanup, daemon=True)
cleanup_thread.start()


# ============================================================================
# Áû¨ÈñìËã±‰ΩúÊñá„É¢„Éº„ÉâÁî®„ÅÆAPI „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà
# ============================================================================

# ============================================================================
# „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„É¢„Éº„ÉâÁî®„ÅÆAPI „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà
# ============================================================================


# „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÅÆÂøúÁ≠î„É¢„Éá„É´
class ListeningProblem(BaseModel):
    """
    „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÅÆ„É¨„Çπ„Éù„É≥„Çπ„É¢„Éá„É´
    """

    question: str  # ÂïèÈ°åÊñáÔºàÈü≥Â£∞„ÅßË™≠„Åø‰∏ä„Åí„ÇãÔºâ
    choices: list  # ÈÅ∏ÊäûËÇ¢„ÅÆ„É™„Çπ„Éà
    correct_answer: str  # Ê≠£Ëß£
    difficulty: str  # Èõ£ÊòìÂ∫¶Ôºàeasy, medium, hardÔºâ
    category: str  # „Ç´„ÉÜ„Ç¥„É™
    explanation: str  # Ëß£Ë™¨Ôºà‰ªªÊÑèÔºâ


class ListeningAnswerRequest(BaseModel):
    """
    „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÅÆÂõûÁ≠î„ÉÅ„Çß„ÉÉ„ÇØÁî®„É™„ÇØ„Ç®„Çπ„Éà„É¢„Éá„É´
    """

    question: str  # ÂïèÈ°åÊñá
    user_answer: str  # „É¶„Éº„Ç∂„Éº„ÅÆÂõûÁ≠î
    correct_answer: str  # Ê≠£Ëß£
    choices: list  # ÈÅ∏ÊäûËÇ¢


class ListeningAnswerResponse(BaseModel):
    """
    „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÅÆÂõûÁ≠î„ÉÅ„Çß„ÉÉ„ÇØÁî®„É¨„Çπ„Éù„É≥„Çπ„É¢„Éá„É´
    """

    is_correct: bool  # Ê≠£Ëß£„Åã„Å©„ÅÜ„Åã
    feedback: str  # „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ
    explanation: str  # Ëß£Ë™¨


# Áû¨ÈñìËã±‰ΩúÊñá„ÅÆÂïèÈ°å„Éë„Çø„Éº„É≥
TRANSLATION_PROBLEMS = [
    {
        "japanese": "‰ªäÊó•„ÅØÂ§©Ê∞ó„Åå„ÅÑ„ÅÑ„Åß„Åô„Å≠„ÄÇ",
        "english": "It's nice weather today.",
        "difficulty": "easy",
        "category": "weather",
    },
    {
        "japanese": "Êò®Êó•„ÄÅÂèãÈÅî„Å®Êò†Áîª„ÇíË¶ã„Å´Ë°å„Åç„Åæ„Åó„Åü„ÄÇ",
        "english": "I went to see a movie with my friend yesterday.",
        "difficulty": "medium",
        "category": "daily_life",
    },
    {
        "japanese": "Êù•ÈÄ±„ÅÆÈáëÊõúÊó•„Å´‰ºöË≠∞„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
        "english": "There will be a meeting next Friday.",
        "difficulty": "medium",
        "category": "business",
    },
    {
        "japanese": "„ÇÇ„Åó„ÇÇÊôÇÈñì„Åå„ÅÇ„Çå„Å∞„ÄÅ‰∏ÄÁ∑í„Å´Ë≤∑„ÅÑÁâ©„Å´Ë°å„Åç„Åæ„Åõ„Çì„ÅãÔºü",
        "english": "If you have time, would you like to go shopping together?",
        "difficulty": "hard",
        "category": "invitation",
    },
    {
        "japanese": "ÂΩºÂ•≥„ÅØÊØéÊúù7ÊôÇ„Å´Ëµ∑„Åç„Åæ„Åô„ÄÇ",
        "english": "She gets up at 7 o'clock every morning.",
        "difficulty": "easy",
        "category": "daily_routine",
    },
    {
        "japanese": "„Åì„ÅÆÊú¨„ÅØÁßÅ„Å´„Å®„Å£„Å¶Èõ£„Åó„Åô„Åé„Åæ„Åô„ÄÇ",
        "english": "This book is too difficult for me.",
        "difficulty": "medium",
        "category": "opinion",
    },
    {
        "japanese": "ÈõªËªä„ÅåÈÅÖ„Çå„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅÂ∞ë„ÅóÈÅÖ„Çå„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ",
        "english": "The train is delayed, so I might be a little late.",
        "difficulty": "hard",
        "category": "transportation",
    },
    {
        "japanese": "Â§è‰ºë„Åø„Å´ÂÆ∂Êóè„Å®ÂåóÊµ∑ÈÅì„Å´Ë°å„Åè‰∫àÂÆö„Åß„Åô„ÄÇ",
        "english": "I'm planning to go to Hokkaido with my family during summer vacation.",
        "difficulty": "medium",
        "category": "travel",
    },
    {
        "japanese": "Êó•Êú¨Ë™û„ÇíÂãâÂº∑„Åô„Çã„ÅÆ„ÅØÊ•Ω„Åó„ÅÑ„Åß„Åô„ÄÇ",
        "english": "Studying Japanese is fun.",
        "difficulty": "easy",
        "category": "learning",
    },
    {
        "japanese": "„ÇÇ„ÅóÈõ®„ÅåÈôç„Å£„Åü„Çâ„ÄÅÂÆ∂„Å´„ÅÑ„Çã„Å§„ÇÇ„Çä„Åß„Åô„ÄÇ",
        "english": "If it rains, I intend to stay home.",
        "difficulty": "hard",
        "category": "conditional",
    },
    # ËøΩÂä†„ÅÆwork/business„Ç´„ÉÜ„Ç¥„É™ÂïèÈ°å
    {
        "japanese": "„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÈÄ≤Êçó„ÅØ„ÅÑ„Åã„Åå„Åß„Åô„ÅãÔºü",
        "english": "How is the progress of the project?",
        "difficulty": "medium",
        "category": "work",
    },
    {
        "japanese": "Êù•Êúà„Åã„ÇâÊñ∞„Åó„ÅÑÈÉ®ÁΩ≤„Å´Áï∞Âãï„Åô„Çã„Åì„Å®„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ",
        "english": "I will be transferred to a new department starting next month.",
        "difficulty": "hard",
        "category": "work",
    },
    {
        "japanese": "„Åì„ÅÆÊèêÊ°àÊõ∏„Å´„Å§„ÅÑ„Å¶Ë≥™Âïè„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
        "english": "I have a question about this proposal.",
        "difficulty": "medium",
        "category": "work",
    },
    {
        "japanese": "‰ºöË≠∞„ÅÆË≥áÊñô„ÇíÊ∫ñÂÇô„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
        "english": "I need to prepare materials for the meeting.",
        "difficulty": "easy",
        "category": "work",
    },
    {
        "japanese": "Á∑†Âàá„ÇíÂª∂Èï∑„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Åì„Å®„ÅØÂèØËÉΩ„Åß„Åó„Çá„ÅÜ„ÅãÔºü",
        "english": "Would it be possible to extend the deadline?",
        "difficulty": "hard",
        "category": "work",
    },
    # technology „Ç´„ÉÜ„Ç¥„É™
    {
        "japanese": "Êñ∞„Åó„ÅÑ„Ç¢„Éó„É™„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åó„Åü„ÄÇ",
        "english": "I downloaded a new app.",
        "difficulty": "easy",
        "category": "technology",
    },
    {
        "japanese": "„Ç≥„É≥„Éî„É•„Éº„Çø„Éº„ÅåÂãï„Åã„Å™„Åè„Å™„Å£„Å¶„Åó„Åæ„ÅÑ„Åæ„Åó„Åü„ÄÇ",
        "english": "My computer has stopped working.",
        "difficulty": "medium",
        "category": "technology",
    },
    {
        "japanese": "„Åì„ÅÆ„ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„ÅØÈùûÂ∏∏„Å´‰Ωø„ÅÑ„ÇÑ„Åô„ÅÑ„Åß„Åô„ÄÇ",
        "english": "This software is very user-friendly.",
        "difficulty": "medium",
        "category": "technology",
    },
    # health „Ç´„ÉÜ„Ç¥„É™
    {
        "japanese": "È†≠„ÅåÁóõ„ÅÑ„ÅÆ„ÅßÁóÖÈô¢„Å´Ë°å„Åç„Åæ„Åô„ÄÇ",
        "english": "I have a headache, so I'm going to the hospital.",
        "difficulty": "easy",
        "category": "health",
    },
    {
        "japanese": "ÊØéÊó•ÈÅãÂãï„Åô„Çã„Çà„ÅÜ„Å´ÂøÉ„Åå„Åë„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
        "english": "I try to exercise every day.",
        "difficulty": "medium",
        "category": "health",
    },
    {
        "japanese": "„Éê„É©„É≥„Çπ„ÅÆÂèñ„Çå„ÅüÈ£ü‰∫ã„ÇíÊëÇ„Çã„Åì„Å®„ÅåÂ§ßÂàá„Åß„Åô„ÄÇ",
        "english": "It's important to have a balanced diet.",
        "difficulty": "hard",
        "category": "health",
    },
    # education „Ç´„ÉÜ„Ç¥„É™
    {
        "japanese": "Â§ßÂ≠¶„ÅßÁµåÊ∏àÂ≠¶„ÇíÂ∞ÇÊîª„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
        "english": "I'm majoring in economics at university.",
        "difficulty": "medium",
        "category": "education",
    },
    {
        "japanese": "Âõ≥Êõ∏È§®„ÅßÂÆøÈ°å„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
        "english": "I'm doing my homework at the library.",
        "difficulty": "easy",
        "category": "education",
    },
    {
        "japanese": "‰ªäÂ∫¶„ÅÆË©¶È®ì„ÅÆÊ∫ñÂÇô„Çí„Åó„Å™„Åë„Çå„Å∞„Å™„Çä„Åæ„Åõ„Çì„ÄÇ",
        "english": "I have to prepare for the upcoming exam.",
        "difficulty": "medium",
        "category": "education",
    },
]


@app.get(
    "/api/instant-translation/problem",
    response_model=InstantTranslationProblem,
)
async def get_instant_translation_problem(
    difficulty: str = "all",
    category: str = "all",
    eiken_level: str = "",
    long_text_mode: bool = False,
):
    """
    Áû¨ÈñìËã±‰ΩúÊñá„ÅÆÂïèÈ°å„ÇíÂèñÂæó„Åô„ÇãAPI„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà

    Èõ£ÊòìÂ∫¶„ÄÅ„Ç´„ÉÜ„Ç¥„É™„ÄÅËã±Ê§ú„É¨„Éô„É´„Å´Âü∫„Å•„ÅÑ„Å¶ÈÅ©Âàá„Å™ÂïèÈ°å„ÇíËøî„Åó„Åæ„Åô„ÄÇ
    Ëã±Ê§ú„É¨„Éô„É´„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„ÄÅAI„Çí‰Ωø„Å£„Å¶ÂãïÁöÑ„Å´ÂïèÈ°å„ÇíÁîüÊàê„Åó„Åæ„Åô„ÄÇ

    Args:
        difficulty: ÂïèÈ°å„ÅÆÈõ£ÊòìÂ∫¶ (all, basic, intermediate, advanced)
        category: ÂïèÈ°å„ÅÆ„Ç´„ÉÜ„Ç¥„É™ (all, daily_life, work, travel, etc.)
        eiken_level: Ëã±Ê§ú„É¨„Éô„É´ (5, 4, 3, pre-2, 2, pre-1, 1)
    """

    print(
        f"üîî Instant translation problem request: difficulty={difficulty}, category={category}, eiken_level={eiken_level}, long_text_mode={long_text_mode}"
    )

    try:
        import json
        import random

        # Ëã±Ê§ú„É¨„Éô„É´„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Å¶„ÄÅAI„ÅåÂà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà„ÅØAIÁîüÊàê„ÇíË©¶Ë°å
        if eiken_level and eiken_level.strip() and model:
            print(f"ü§ñ Generating AI problem for Eiken level {eiken_level}")

            try:
                # „Ç´„ÉÜ„Ç¥„É™„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞
                category_for_ai = category if category != "all" else "general"

                # AIÂïèÈ°åÁîüÊàê„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàê
                ai_prompt = create_eiken_problem_generation_prompt(
                    eiken_level, category_for_ai, long_text_mode
                )

                # AI„Å´ÂïèÈ°åÁîüÊàê„Çí‰æùÈ†ºÔºàÈùûÂêåÊúüÂÆüË°åÔºâ
                loop = asyncio.get_event_loop()
                ai_response = await loop.run_in_executor(
                    executor, lambda: model.generate_content(ai_prompt)
                )

                if ai_response.text:
                    # AI„ÅÆÂøúÁ≠î„Çí„Éë„Éº„Çπ„Åó„Å¶JSON„ÇíÊäΩÂá∫
                    ai_text = ai_response.text.strip()

                    # JSON„Éñ„É≠„ÉÉ„ÇØ„ÇíÊé¢„Åô
                    json_start = ai_text.find("{")
                    json_end = ai_text.rfind("}") + 1

                    if json_start != -1 and json_end > json_start:
                        json_text = ai_text[json_start:json_end]

                        try:
                            ai_problem = json.loads(json_text)

                            # ÂøÖË¶Å„Å™„Éï„Ç£„Éº„É´„Éâ„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                            if all(
                                key in ai_problem
                                for key in ["japanese", "english"]
                            ):
                                print(f"‚úÖ AI generated problem successfully")

                                # Èõ£ÊòìÂ∫¶„Å®„Ç´„ÉÜ„Ç¥„É™„ÇíË™øÊï¥
                                eiken_to_difficulty = {
                                    "5": "easy",
                                    "4": "easy",
                                    "3": "medium",
                                    "pre-2": "medium",
                                    "2": "medium",
                                    "pre-1": "hard",
                                    "1": "hard",
                                }

                                return InstantTranslationProblem(
                                    japanese=ai_problem["japanese"],
                                    english=ai_problem["english"],
                                    difficulty=ai_problem.get(
                                        "difficulty",
                                        eiken_to_difficulty.get(
                                            eiken_level, "medium"
                                        ),
                                    ),
                                    category=ai_problem.get(
                                        "category", category_for_ai
                                    ),
                                )
                            else:
                                print(
                                    f"‚ö†Ô∏è AI response missing required fields, falling back to static problems"
                                )
                        except json.JSONDecodeError as e:
                            print(
                                f"‚ö†Ô∏è Failed to parse AI JSON response: {e}, falling back to static problems"
                            )
                    else:
                        print(
                            f"‚ö†Ô∏è No valid JSON found in AI response, falling back to static problems"
                        )
                else:
                    print(
                        f"‚ö†Ô∏è Empty AI response, falling back to static problems"
                    )

            except Exception as e:
                print(
                    f"‚ö†Ô∏è AI problem generation failed: {e}, falling back to static problems"
                )

        # ÈùôÁöÑÂïèÈ°å„É™„Çπ„Éà„Åã„Çâ„ÅÆÈÅ∏ÊäûÔºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
        print(f"üìö Using static problem list")

        # Ëã±Ê§ú„É¨„Éô„É´„ÇíÈõ£ÊòìÂ∫¶„Å´„Éû„ÉÉ„Éî„É≥„Ç∞
        eiken_to_difficulty = {
            "5": "easy",
            "4": "easy",
            "3": "medium",
            "pre-2": "medium",
            "2": "medium",
            "pre-1": "hard",
            "1": "hard",
        }

        # Èõ£ÊòìÂ∫¶„ÅÆÊ±∫ÂÆö - Ëã±Ê§ú„É¨„Éô„É´„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÂÑ™ÂÖà
        if eiken_level and eiken_level in eiken_to_difficulty:
            target_difficulty = eiken_to_difficulty[eiken_level]
        elif difficulty != "all":
            # „Éï„É≠„É≥„Éà„Ç®„É≥„Éâ„ÅÆÈõ£ÊòìÂ∫¶„Çí„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅÆÂΩ¢Âºè„Å´Â§âÊèõ
            difficulty_mapping = {
                "basic": "easy",
                "intermediate": "medium",
                "advanced": "hard",
            }
            target_difficulty = difficulty_mapping.get(difficulty, "medium")
        else:
            target_difficulty = "all"

        # ÂïèÈ°å„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
        filtered_problems = []

        # Èõ£ÊòìÂ∫¶„Éï„Ç£„É´„Çø
        if target_difficulty == "all":
            filtered_problems = TRANSLATION_PROBLEMS.copy()
        else:
            filtered_problems = [
                p
                for p in TRANSLATION_PROBLEMS
                if p["difficulty"] == target_difficulty
            ]

        # „Ç´„ÉÜ„Ç¥„É™„Éï„Ç£„É´„Çø
        if category != "all":
            # „Éï„É≠„É≥„Éà„Ç®„É≥„Éâ„ÅÆ„Ç´„ÉÜ„Ç¥„É™Âêç„Çí„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅÆÂΩ¢Âºè„Å´Â§âÊèõ
            category_mapping = {
                "daily_life": ["daily_life", "daily_routine", "preferences"],
                "work": ["business", "work"],
                "travel": ["travel", "transportation"],
                "education": ["learning", "education"],
                "technology": ["technology"],
                "health": ["health"],
                "culture": ["general"],  # ‰ªäÂæåËøΩÂä†‰∫àÂÆö
                "environment": ["general"],  # ‰ªäÂæåËøΩÂä†‰∫àÂÆö
            }

            target_categories = category_mapping.get(category, [category])
            filtered_problems = [
                p
                for p in filtered_problems
                if p["category"] in target_categories
            ]
        # Âà©Áî®ÂèØËÉΩ„Å™ÂïèÈ°å„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        if not filtered_problems:
            print(f"No problems found for filters, using fallback")
            filtered_problems = TRANSLATION_PROBLEMS.copy()

        # „É©„É≥„ÉÄ„É†„Å´ÂïèÈ°å„ÇíÈÅ∏Êäû
        problem = random.choice(filtered_problems)

        return InstantTranslationProblem(
            japanese=problem["japanese"],
            english=problem["english"],
            difficulty=problem["difficulty"],
            category=problem["category"],
        )

    except Exception as e:
        print(f"Error generating instant translation problem: {str(e)}")
        # „Ç®„É©„ÉºÊôÇ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂïèÈ°å
        fallback_problem = {
            "japanese": "ÁßÅ„ÅØÊØéÊó•Ëã±Ë™û„ÇíÂãâÂº∑„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
            "english": "I study English every day.",
            "difficulty": "easy",
            "category": "daily_life",
        }

        return InstantTranslationProblem(
            japanese=fallback_problem["japanese"],
            english=fallback_problem["english"],
            difficulty=fallback_problem["difficulty"],
            category=fallback_problem["category"],
        )


# ============================================================================
# „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°åÂèñÂæó„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà
# ============================================================================


@app.get("/api/listening/problem", response_model=ListeningProblem)
async def get_listening_problem(
    category: str = "any",
    difficulty: str = "medium",
    _t: str = None,  # „Ç≠„É£„ÉÉ„Ç∑„É•„Éê„Çπ„ÉÜ„Ç£„É≥„Ç∞Áî®„Çø„Ç§„É†„Çπ„Çø„É≥„Éó„Éë„É©„É°„Éº„ÇøÔºà‰ΩøÁî®„Åó„Å™„ÅÑÔºâ
):
    """
    Trivia API„Çí‰ΩøÁî®„Åó„Å¶„É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÇíÂèñÂæó„Åô„Çã„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà

    Args:
        category: ÂïèÈ°å„ÅÆ„Ç´„ÉÜ„Ç¥„É™ (any, sports, science, history, etc.)
        difficulty: Èõ£ÊòìÂ∫¶ (easy, medium, hard)
        _t: „Ç≠„É£„ÉÉ„Ç∑„É•„Éê„Çπ„ÉÜ„Ç£„É≥„Ç∞Áî®„Çø„Ç§„É†„Çπ„Çø„É≥„ÉóÔºàÂÜÖÈÉ®„Åß„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑÔºâ

    Returns:
        ListeningProblem: ÂïèÈ°åÊñá„ÄÅÈÅ∏ÊäûËÇ¢„ÄÅÊ≠£Ëß£„ÄÅÈõ£ÊòìÂ∫¶„ÄÅ„Ç´„ÉÜ„Ç¥„É™„ÇíÂê´„ÇÄ
    """
    try:
        import httpx

        # Open Trivia Database API„ÅÆ„Éë„É©„É°„Éº„ÇøË®≠ÂÆö
        base_url = "https://opentdb.com/api.php"
        params = {
            "amount": 1,  # 1ÂïèÂèñÂæó
            "type": "multiple",  # Â§öËÇ¢ÈÅ∏ÊäûÂïèÈ°å
            "difficulty": difficulty,
            "encode": "url3986",  # RFC 3986 URL „Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞
        }

        # „Ç´„ÉÜ„Ç¥„É™„Éû„ÉÉ„Éî„É≥„Ç∞ÔºàOpen Trivia DB„ÅÆ„Ç´„ÉÜ„Ç¥„É™IDÔºâ
        category_mapping = {
            "any": None,
            "general": 9,
            "books": 10,
            "film": 11,
            "music": 12,
            "television": 14,
            "science": 17,
            "computers": 18,
            "math": 19,
            "mythology": 20,
            "sports": 21,
            "geography": 22,
            "history": 23,
            "politics": 24,
            "art": 25,
            "celebrities": 26,
            "animals": 27,
            "vehicles": 28,
        }

        # „Ç´„ÉÜ„Ç¥„É™„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„Éë„É©„É°„Éº„Çø„Å´ËøΩÂä†
        if category != "any" and category in category_mapping:
            params["category"] = category_mapping[category]

        # Trivia API„Åã„ÇâÂïèÈ°å„ÇíÂèñÂæóÔºà„É¨„Éº„ÉàÂà∂ÈôêËÄÉÊÖÆÔºâ
        import asyncio
        import time

        # „É¨„Éº„ÉàÂà∂Èôê„ÉÅ„Çß„ÉÉ„ÇØÔºà5ÁßíÈñìÈöîÔºâ
        current_time = time.time()
        if hasattr(get_listening_problem, "_last_request_time"):
            time_since_last = (
                current_time - get_listening_problem._last_request_time
            )
            if time_since_last < 5.0:
                wait_time = 5.0 - time_since_last
                print(f"‚è≥ Rate limit: waiting {wait_time:.1f} seconds")
                await asyncio.sleep(wait_time)

        get_listening_problem._last_request_time = time.time()

        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()

        # „É¨„Çπ„Éù„É≥„Çπ„Ç≥„Éº„Éâ„ÉÅ„Çß„ÉÉ„ÇØ
        response_code = data.get("response_code", -1)

        if response_code == 1:
            raise Exception(
                "API Error: Not enough questions for the specified criteria"
            )
        elif response_code == 2:
            raise Exception("API Error: Invalid parameters")
        elif response_code == 3:
            raise Exception("API Error: Token not found")
        elif response_code == 4:
            raise Exception("API Error: Token exhausted")
        elif response_code == 5:
            raise Exception("API Error: Rate limit exceeded")
        elif response_code != 0:
            raise Exception(
                f"API Error: Unknown response code {response_code}"
            )

        if not data.get("results"):
            raise Exception("No questions returned from API")

        # ÂïèÈ°å„Éá„Éº„Çø„ÇíÊäΩÂá∫
        question_data = data["results"][0]

        # URL „Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Çí„Éá„Ç≥„Éº„Éâ
        import urllib.parse

        question = urllib.parse.unquote(question_data["question"])
        correct_answer = urllib.parse.unquote(question_data["correct_answer"])
        incorrect_answers = [
            urllib.parse.unquote(ans)
            for ans in question_data["incorrect_answers"]
        ]

        # ÈÅ∏ÊäûËÇ¢„Çí„Ç∑„É£„ÉÉ„Éï„É´
        import random

        choices = [correct_answer] + incorrect_answers
        random.shuffle(choices)

        return ListeningProblem(
            question=question,
            choices=choices,
            correct_answer=correct_answer,
            difficulty=question_data["difficulty"],
            category=question_data["category"],
            explanation="",  # Trivia API„Å´„ÅØËß£Ë™¨„Åå„Å™„ÅÑ„Åü„ÇÅÁ©∫ÊñáÂ≠ó
        )

    except Exception as e:
        print(f"Error fetching listening problem: {str(e)}")

        # ÂÖÖÂÆü„Åó„Åü„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂïèÈ°å„Çª„ÉÉ„Éà
        fallback_problems = [
            {
                "question": "What is the capital of Japan?",
                "choices": ["Tokyo", "Osaka", "Kyoto", "Hiroshima"],
                "correct_answer": "Tokyo",
                "difficulty": "easy",
                "category": "Geography",
            },
            {
                "question": "Which planet is known as the Red Planet?",
                "choices": ["Venus", "Mars", "Jupiter", "Saturn"],
                "correct_answer": "Mars",
                "difficulty": "easy",
                "category": "Science",
            },
            {
                "question": "How many continents are there on Earth?",
                "choices": ["5", "6", "7", "8"],
                "correct_answer": "7",
                "difficulty": "medium",
                "category": "Geography",
            },
            {
                "question": "What is the largest mammal in the world?",
                "choices": [
                    "Elephant",
                    "Blue Whale",
                    "Giraffe",
                    "Hippopotamus",
                ],
                "correct_answer": "Blue Whale",
                "difficulty": "medium",
                "category": "Science",
            },
        ]

        # Èõ£ÊòìÂ∫¶„Å´Âøú„Åò„Å¶„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂïèÈ°å„ÇíÈÅ∏Êäû
        suitable_problems = [
            p for p in fallback_problems if p["difficulty"] == difficulty
        ]
        if not suitable_problems:
            suitable_problems = (
                fallback_problems  # ÈÅ©Âàá„Å™Èõ£ÊòìÂ∫¶„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÂÖ®„Å¶
            )

        import random

        selected_problem = random.choice(suitable_problems)

        return ListeningProblem(
            question=selected_problem["question"],
            choices=selected_problem["choices"],
            correct_answer=selected_problem["correct_answer"],
            difficulty=selected_problem["difficulty"],
            category=selected_problem["category"],
            explanation="This is a fallback question due to external API issues.",
        )


# ============================================================================
# „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°åÂõûÁ≠î„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà
# ============================================================================


@app.post("/api/listening/check", response_model=ListeningAnswerResponse)
async def check_listening_answer(req: ListeningAnswerRequest):
    """
    „É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÅÆÂõûÁ≠î„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„ÄÅ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÊèê‰æõ„Åô„Çã„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà

    Args:
        req: „É¶„Éº„Ç∂„Éº„ÅÆÂõûÁ≠î„Éá„Éº„Çø

    Returns:
        ListeningAnswerResponse: Ê≠£Ëß£Âà§ÂÆö„ÄÅ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÄÅËß£Ë™¨
    """
    try:
        # Ê≠£Ëß£Âà§ÂÆöÔºàÂ§ßÊñáÂ≠óÂ∞èÊñáÂ≠ó„ÇíÁÑ°Ë¶ñÔºâ
        is_correct = (
            req.user_answer.strip().lower()
            == req.correct_answer.strip().lower()
        )

        # AI„Çí‰ΩøÁî®„Åó„Å¶„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÁîüÊàê
        if model:
            prompt = f"""
„ÅÇ„Å™„Åü„ÅØËã±Ë™ûÂ≠¶ÁøíËÄÖÂêë„Åë„ÅÆ„É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÉÅ„É•„Éº„Çø„Éº„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆ„É™„Çπ„Éã„É≥„Ç∞ÂïèÈ°å„ÅÆÂõûÁ≠î„Å´„Å§„ÅÑ„Å¶„ÄÅÂä±„Åæ„Åó„Å®„Å®„ÇÇ„Å´„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

ÂïèÈ°å: {req.question}
ÈÅ∏ÊäûËÇ¢: {', '.join(req.choices)}
Ê≠£Ëß£: {req.correct_answer}
„É¶„Éº„Ç∂„Éº„ÅÆÂõûÁ≠î: {req.user_answer}
Ê≠£Ëß£Âà§ÂÆö: {'Ê≠£Ëß£' if is_correct else '‰∏çÊ≠£Ëß£'}

„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÅØ‰ª•‰∏ã„ÅÆË¶ÅÁ¥†„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºö
1. Ê≠£Ëß£„Éª‰∏çÊ≠£Ëß£„ÅÆÂà§ÂÆö
2. Ê≠£Ëß£„ÅÆÁêÜÁî±„ÇÑËÉåÊôØÁü•Ë≠ò
3. Â≠¶ÁøíËÄÖ„Å∏„ÅÆÂä±„Åæ„Åó„ÅÆË®ÄËëâ
4. Êó•Êú¨Ë™û„Åß100ÊñáÂ≠ó‰ª•ÂÜÖ

ÂõûÁ≠îÂΩ¢ÂºèÔºöJSON
{{
    "feedback": "„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÊñá",
    "explanation": "Ëß£Ë™¨Êñá"
}}
"""

            try:
                ai_response = model.generate_content(prompt)
                if ai_response.text:
                    import json
                    import re

                    # JSON„ÇíÊäΩÂá∫
                    json_match = re.search(
                        r"\{.*\}", ai_response.text, re.DOTALL
                    )
                    if json_match:
                        response_data = json.loads(json_match.group())
                        feedback = response_data.get("feedback", "")
                        explanation = response_data.get("explanation", "")
                    else:
                        raise Exception("No JSON found in AI response")
                else:
                    raise Exception("Empty AI response")

            except Exception as e:
                print(f"AI feedback generation failed: {e}")
                # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ
                if is_correct:
                    feedback = "Ê≠£Ëß£„Åß„ÅôÔºÅ„Çà„Åè„Åß„Åç„Åæ„Åó„Åü„ÄÇ"
                    explanation = f"Á≠î„Åà„ÅØ„Äå{req.correct_answer}„Äç„Åß„Åô„ÄÇ"
                else:
                    feedback = (
                        f"ÊÉú„Åó„ÅÑÔºÅÊ≠£Ëß£„ÅØ„Äå{req.correct_answer}„Äç„Åß„Åó„Åü„ÄÇ"
                    )
                    explanation = "Ê¨°Âõû„ÇÇÈ†ëÂºµ„Çä„Åæ„Åó„Çá„ÅÜÔºÅ"
        else:
            # Gemini API„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ„Ç∑„É≥„Éó„É´„Å™„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ
            if is_correct:
                feedback = "Ê≠£Ëß£„Åß„ÅôÔºÅÁ¥†Êô¥„Çâ„Åó„ÅÑÔºÅ"
                explanation = f"Á≠î„Åà„ÅØ„Äå{req.correct_answer}„Äç„Åß„Åô„ÄÇ"
            else:
                feedback = (
                    f"‰∏çÊ≠£Ëß£„Åß„Åô„ÄÇÊ≠£Ëß£„ÅØ„Äå{req.correct_answer}„Äç„Åß„Åó„Åü„ÄÇ"
                )
                explanation = "Ê¨°Âõû„ÇÇÈ†ëÂºµ„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºÅ"

        return ListeningAnswerResponse(
            is_correct=is_correct, feedback=feedback, explanation=explanation
        )

    except Exception as e:
        print(f"Error checking listening answer: {str(e)}")
        return ListeningAnswerResponse(
            is_correct=False,
            feedback="ÂõûÁ≠î„ÅÆÁ¢∫Ë™ç‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ",
            explanation="„ÇÇ„ÅÜ‰∏ÄÂ∫¶„ÅäË©¶„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ",
        )


@app.post(
    "/api/instant-translation/check",
    response_model=InstantTranslationCheckResponse,
)
async def check_instant_translation_answer(
    req: InstantTranslationCheckRequest,
):
    """
    Áû¨ÈñìËã±‰ΩúÊñá„ÅÆÂõûÁ≠î„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åô„ÇãAPI„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà

    „É¶„Éº„Ç∂„Éº„ÅÆÂõûÁ≠î„ÇíÊ≠£Ëß£„Å®ÊØîËºÉ„Åó„ÄÅAI„Çí‰Ωø„Å£„Å¶Ë©≥Á¥∞„Å™„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ
    """

    print(f"üîî Instant translation check request: '{req.userAnswer[:30]}...'")

    try:
        if not model:
            # Gemini API„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ„Ç∑„É≥„Éó„É´„Å™ÊØîËºÉ
            is_correct = (
                req.userAnswer.lower().strip()
                == req.correctAnswer.lower().strip()
            )
            return InstantTranslationCheckResponse(
                isCorrect=is_correct,
                feedback=(
                    "Good try! Keep practicing."
                    if is_correct
                    else "Close, but not quite right. Try again!"
                ),
                score=100 if is_correct else 50,
                suggestions=[],
            )

        # AI„Çí‰Ωø„Å£„Å¶Ë©≥Á¥∞„Å™ÂõûÁ≠î„ÉÅ„Çß„ÉÉ„ÇØÔºàÈùûÂêåÊúüÂÆüË°åÔºâ
        check_prompt = create_translation_check_prompt(
            req.japanese, req.correctAnswer, req.userAnswer
        )

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            executor, lambda: model.generate_content(check_prompt)
        )

        if response.text:
            # AIÂøúÁ≠î„Åã„ÇâÊÉÖÂ†±„ÇíÊäΩÂá∫
            ai_feedback = response.text

            # Á∞°Âçò„Å™Ê≠£Ëß£Âà§ÂÆöÔºàAI„ÅÆÂøúÁ≠î„Å´Âü∫„Å•„ÅèÔºâ
            is_correct = any(
                word in ai_feedback.lower()
                for word in ["correct", "good", "excellent", "right"]
            )

            # „Çπ„Ç≥„Ç¢Ë®àÁÆóÔºàÁ∞°Âçò„Å™ÂÆüË£ÖÔºâ
            score = 100 if is_correct else 70

            return InstantTranslationCheckResponse(
                isCorrect=is_correct,
                feedback=ai_feedback,
                score=score,
                suggestions=[],
            )
        else:
            # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÂøúÁ≠î
            return InstantTranslationCheckResponse(
                isCorrect=False,
                feedback="Sorry, I couldn't evaluate your answer properly. Please try again.",
                score=50,
                suggestions=[],
            )

    except Exception as e:
        print(f"Error checking instant translation answer: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="Failed to check instant translation answer",
        )


def create_translation_check_prompt(
    japanese: str, correct_answer: str, user_answer: str
) -> str:
    """
    Áû¨ÈñìËã±‰ΩúÊñá„ÅÆÂõûÁ≠î„ÉÅ„Çß„ÉÉ„ÇØÁî®„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàê

    Args:
        japanese: Êó•Êú¨Ë™û„ÅÆÂéüÊñá
        correct_answer: Ê≠£Ëß£„ÅÆËã±Ë™û
        user_answer: „É¶„Éº„Ç∂„Éº„ÅÆÂõûÁ≠î

    Returns:
        AI„ÅåÂõûÁ≠î„ÇíË©ï‰æ°„Åô„Çã„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
    """

    prompt = f"""
„ÅÇ„Å™„Åü„ÅØÁµåÈ®ìË±äÂØå„Å™Ëã±Ë™ûÊïôÂ∏´„Åß„Åô„ÄÇÊó•Êú¨‰∫∫Â≠¶ÁøíËÄÖ„ÅÆÁû¨ÈñìËã±‰ΩúÊñá„ÅÆÂõûÁ≠î„ÇíË©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÂïèÈ°å„Äë
Êó•Êú¨Ë™û: "{japanese}"
Ê≠£Ëß£: "{correct_answer}"
Â≠¶ÁøíËÄÖ„ÅÆÂõûÁ≠î: "{user_answer}"

„ÄêË©ï‰æ°Âü∫Ê∫ñ„Äë
- ÊÑèÂë≥„ÅåÊ≠£Á¢∫„Å´‰ºù„Çè„Å£„Å¶„ÅÑ„Çã„Åã
- ÊñáÊ≥ï„ÅåÊ≠£„Åó„ÅÑ„Åã
- Ëá™ÁÑ∂„Å™Ëã±Ë™ûË°®Áèæ„Åã
- Ë™ûÂΩô„ÅÆÈÅ∏Êäû„ÅåÈÅ©Âàá„Åã

„ÄêËøîÁ≠îÂΩ¢Âºè„Äë
‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßË©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- „ÄåExcellent!„Äç„ÄåGood!„Äç„ÄåNot quite right„Äç„ÅÆ„ÅÑ„Åö„Çå„Åã„ÅßÂßã„ÇÅ„Çã
- ÂÖ∑‰ΩìÁöÑ„Å™ÊîπÂñÑÁÇπ„ÇÑ„Ç¢„Éâ„Éê„Ç§„Çπ„ÇíÂê´„ÇÅ„Çã
- Âä±„Åæ„Åó„ÅÆË®ÄËëâ„ÇíÂê´„ÇÅ„Çã
- 2-3Êñá„ÅßÁ∞°ÊΩî„Å´„Åæ„Å®„ÇÅ„Çã

Êó•Êú¨‰∫∫Â≠¶ÁøíËÄÖ„Å´„Å®„Å£„Å¶ÁêÜËß£„Åó„ÇÑ„Åô„Åè„ÄÅÂ≠¶ÁøíÊÑèÊ¨≤„ÇíÈ´ò„ÇÅ„Çã„Çà„ÅÜ„Å™Ë©ï‰æ°„Çí„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ
"""

    return prompt


@app.get(
    "/api/eiken-translation-problem",
    response_model=InstantTranslationProblem,
)
async def get_eiken_translation_problem(
    difficulty: str = "all", category: str = "all", eiken_level: str = ""
):
    """
    Ëã±Ê§úÂØæÂøúÁû¨ÈñìËã±‰ΩúÊñáÂïèÈ°åÂèñÂæóAPI„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà

    „Éï„É≠„É≥„Éà„Ç®„É≥„Éâ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅ„ÅÆ„Ç®„Ç§„É™„Ç¢„Çπ„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà„ÄÇ
    /api/instant-translation/problem„Å®Âêå„ÅòÊ©üËÉΩ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ

    Args:
        difficulty: ÂïèÈ°å„ÅÆÈõ£ÊòìÂ∫¶ (all, basic, intermediate, advanced)
        category: ÂïèÈ°å„ÅÆ„Ç´„ÉÜ„Ç¥„É™ (all, daily_life, work, travel, etc.)
        eiken_level: Ëã±Ê§ú„É¨„Éô„É´ (5, 4, 3, pre-2, 2, pre-1, 1)
    """

    # Êó¢Â≠ò„ÅÆÈñ¢Êï∞„ÇíÂëº„Å≥Âá∫„Åó„Å¶ÈáçË§á„ÇíÈÅø„Åë„Çã
    return await get_instant_translation_problem(
        difficulty, category, eiken_level, False
    )


def create_eiken_problem_generation_prompt(
    eiken_level: str, category: str = "general", long_text_mode: bool = False
) -> str:
    """
    Ëã±Ê§ú„É¨„Éô„É´„Å´Âøú„Åò„ÅüÁû¨ÈñìËã±‰ΩúÊñáÂïèÈ°å„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàê

    Args:
        eiken_level: Ëã±Ê§ú„É¨„Éô„É´ (5, 4, 3, pre-2, 2, pre-1, 1)
        category: ÂïèÈ°å„ÅÆ„Ç´„ÉÜ„Ç¥„É™ (daily_life, work, travel, etc.)

    Returns:
        AI„ÅåÂïèÈ°å„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
    """

    # Ëã±Ê§ú„É¨„Éô„É´Âà•„ÅÆÁâπÂæ¥ÂÆöÁæ©
    eiken_characteristics = {
        "5": {
            "description": "Ëã±Ê§ú5Á¥ö (‰∏≠Â≠¶ÂàùÁ¥ö„É¨„Éô„É´)",
            "grammar": "ÁèæÂú®ÂΩ¢„ÄÅÈÅéÂéªÂΩ¢„ÄÅbeÂãïË©û„ÄÅ‰∏ÄËà¨ÂãïË©û„ÅÆÂü∫Êú¨ÂΩ¢",
            "vocabulary": "‰∏≠Â≠¶1Âπ¥Áîü„É¨„Éô„É´„ÅÆÂü∫Êú¨Ë™ûÂΩô (Á¥Ñ600Ë™û)",
            "sentence_structure": "„Ç∑„É≥„Éó„É´„Å™ÂçòÊñá‰∏≠ÂøÉ",
            "examples": [
                "I am a student.",
                "I go to school.",
                "It is sunny today.",
            ],
        },
        "4": {
            "description": "Ëã±Ê§ú4Á¥ö (‰∏≠Â≠¶‰∏≠Á¥ö„É¨„Éô„É´)",
            "grammar": "Âä©ÂãïË©û (can, will, must)„ÄÅÊú™Êù•ÂΩ¢„ÄÅÈÄ≤Ë°åÂΩ¢",
            "vocabulary": "‰∏≠Â≠¶2Âπ¥Áîü„É¨„Éô„É´„ÅÆË™ûÂΩô (Á¥Ñ1300Ë™û)",
            "sentence_structure": "Âä©ÂãïË©û„ÇíÂê´„ÇÄÊñá„ÄÅÁñëÂïèÊñá„ÉªÂê¶ÂÆöÊñá",
            "examples": [
                "I can play tennis.",
                "Will you help me?",
                "She is reading a book.",
            ],
        },
        "3": {
            "description": "Ëã±Ê§ú3Á¥ö (‰∏≠Â≠¶ÂçíÊ•≠„É¨„Éô„É´)",
            "grammar": "ÂèóÂãïÊÖã„ÄÅÁèæÂú®ÂÆå‰∫Ü„ÄÅ‰∏çÂÆöË©û„ÄÅÂãïÂêçË©û",
            "vocabulary": "‰∏≠Â≠¶3Âπ¥Áîü„É¨„Éô„É´„ÅÆË™ûÂΩô (Á¥Ñ2100Ë™û)",
            "sentence_structure": "Ë§áÊñáÊßãÈÄ†„ÄÅÊé•Á∂öË©û„Çí‰Ωø„Å£„ÅüÊñá",
            "examples": [
                "This book was written by him.",
                "I have been to Tokyo.",
                "I want to learn English.",
            ],
        },
        "pre-2": {
            "description": "Ëã±Ê§úÊ∫ñ2Á¥ö (È´òÊ†°‰∏≠Á¥ö„É¨„Éô„É´)",
            "grammar": "Èñ¢‰øÇ‰ª£ÂêçË©û„ÄÅ‰ªÆÂÆöÊ≥ï„ÅÆÂü∫Êú¨„ÄÅÂàÜË©û",
            "vocabulary": "È´òÊ†°Âü∫Á§é„É¨„Éô„É´„ÅÆË™ûÂΩô (Á¥Ñ3600Ë™û)",
            "sentence_structure": "Èñ¢‰øÇË©û„Çí‰Ωø„Å£„ÅüË§áÊñá„ÄÅ„Çà„ÇäË§áÈõë„Å™ÊßãÈÄ†",
            "examples": [
                "The man who is standing there is my teacher.",
                "If I were you, I would study harder.",
            ],
        },
        "2": {
            "description": "Ëã±Ê§ú2Á¥ö (È´òÊ†°ÂçíÊ•≠„É¨„Éô„É´)",
            "grammar": "‰ªÆÂÆöÊ≥ï„ÄÅË§áÈõë„Å™ÊôÇÂà∂„ÄÅÈ´òÂ∫¶„Å™ÊñáÂûã",
            "vocabulary": "È´òÊ†°ÂçíÊ•≠„É¨„Éô„É´„ÅÆË™ûÂΩô (Á¥Ñ5100Ë™û)",
            "sentence_structure": "Ë§áÈõë„Å™Ë§áÊñá„ÄÅË´ñÁêÜÁöÑ„Å™ÊñáÊßãÈÄ†",
            "examples": [
                "If I had studied harder, I could have passed the exam.",
                "Having finished my homework, I went to bed.",
            ],
        },
        "pre-1": {
            "description": "Ëã±Ê§úÊ∫ñ1Á¥ö (Â§ßÂ≠¶‰∏≠Á¥ö„É¨„Éô„É´)",
            "grammar": "È´òÂ∫¶„Å™ÊñáÊ≥ïÊßãÈÄ†„ÄÅË´ñÁêÜÁöÑË°®Áèæ",
            "vocabulary": "Â§ßÂ≠¶‰∏≠Á¥ö„É¨„Éô„É´„ÅÆË™ûÂΩô (Á¥Ñ7500Ë™û)",
            "sentence_structure": "Â≠¶Ë°ìÁöÑ„Éª„Éì„Ç∏„Éç„ÇπÁöÑË°®Áèæ",
            "examples": [
                "The proposal is likely to be implemented next year.",
                "It is essential that we address this issue promptly.",
            ],
        },
        "1": {
            "description": "Ëã±Ê§ú1Á¥ö (Â§ßÂ≠¶‰∏äÁ¥ö„É¨„Éô„É´)",
            "grammar": "ÊúÄÈ´ò„É¨„Éô„É´„ÅÆÊñáÊ≥ï„ÄÅÊÖ£Áî®Ë°®Áèæ",
            "vocabulary": "Â§ßÂ≠¶‰∏äÁ¥ö„É¨„Éô„É´„ÅÆË™ûÂΩô (Á¥Ñ10000-15000Ë™û)",
            "sentence_structure": "È´òÂ∫¶„Å™Ë´ñÁêÜÊßãÈÄ†„ÄÅÂ∞ÇÈñÄÁöÑË°®Áèæ",
            "examples": [
                "The ramifications of this decision could be far-reaching.",
                "Notwithstanding the challenges, we must persevere.",
            ],
        },
    }

    # „Ç´„ÉÜ„Ç¥„É™Âà•„ÅÆ„Éà„Éî„ÉÉ„ÇØ
    category_topics = {
        "daily_life": ["ÂÆ∂Êóè", "È£ü‰∫ã", "Ë≤∑„ÅÑÁâ©", "Ë∂£Âë≥", "Â§©Ê∞ó"],
        "work": ["‰ªï‰∫ã", "‰ºöË≠∞", "„Éó„É≠„Ç∏„Çß„ÇØ„Éà", "ÂêåÂÉö", "„Çπ„Ç±„Ç∏„É•„Éº„É´"],
        "travel": ["ÊóÖË°å", "‰∫§ÈÄö", "ÂÆøÊ≥ä", "Ë¶≥ÂÖâ", "ÊñáÂåñ"],
        "education": ["Â≠¶Ê†°", "ÂãâÂº∑", "Ë©¶È®ì", "Âõ≥Êõ∏È§®", "ÊéàÊ•≠"],
        "health": ["ÂÅ•Â∫∑", "ÁóÖÊ∞ó", "ÈÅãÂãï", "È£ü‰∫ã", "ÁóÖÈô¢"],
        "technology": [
            "„Ç≥„É≥„Éî„É•„Éº„Çø„Éº",
            "„Çπ„Éû„Éº„Éà„Éï„Ç©„É≥",
            "„Ç§„É≥„Çø„Éº„Éç„ÉÉ„Éà",
            "„Ç¢„Éó„É™",
            "SNS",
        ],
        "general": ["‰∏ÄËà¨ÁöÑ„Å™Ë©±È°å", "Êó•Â∏∏ÁöÑ„Å™Ë°®Áèæ", "Âü∫Êú¨ÁöÑ„Å™‰ºöË©±"],
    }

    eiken_info = eiken_characteristics.get(
        eiken_level, eiken_characteristics["3"]
    )
    topics = category_topics.get(category, category_topics["general"])

    prompt = f"""
„ÅÇ„Å™„Åü„ÅØËã±Ê§úÂØæÁ≠ñ„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÊù°‰ª∂„Å´Âæì„Å£„Å¶Áû¨ÈñìËã±‰ΩúÊñá„ÅÆÂïèÈ°å„Çí1„Å§‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÂØæË±°„É¨„Éô„É´„Äë
{eiken_info['description']}

„ÄêÊñáÊ≥ï„É¨„Éô„É´„Äë
{eiken_info['grammar']}

„ÄêË™ûÂΩô„É¨„Éô„É´„Äë
{eiken_info['vocabulary']}

„ÄêÊñáÊßãÈÄ†„Äë
{eiken_info['sentence_structure']}

„ÄêÂèÇËÄÉ‰æãÊñá„Äë
{', '.join(eiken_info['examples'])}

„ÄêÂïèÈ°å„Ç´„ÉÜ„Ç¥„É™„Äë
{category} - „Éà„Éî„ÉÉ„ÇØ‰æã: {', '.join(topics)}

„Äê‰ΩúÊàêÊù°‰ª∂„Äë
1. ÊåáÂÆö„Åï„Çå„ÅüËã±Ê§ú„É¨„Éô„É´„Å´ÈÅ©„Åó„ÅüË™ûÂΩô„ÉªÊñáÊ≥ï„ÅÆ„Åø„Çí‰ΩøÁî®
2. Êó•Êú¨‰∫∫Â≠¶ÁøíËÄÖ„Å´„Å®„Å£„Å¶ÂÆüÁî®ÊÄß„ÅÆÈ´ò„ÅÑË°®Áèæ
3. Ëá™ÁÑ∂„ÅßÈÅ©Âàá„Å™Ëã±Ë™ûË°®Áèæ
4. ÊåáÂÆö„Åï„Çå„Åü„Ç´„ÉÜ„Ç¥„É™„Å´Èñ¢ÈÄ£„Åô„ÇãÂÜÖÂÆπ
5. {"Ë§áÊï∞Êñá„ÅßÊßãÊàê„Åô„ÇãÔºàÈï∑Êñá„É¢„Éº„ÉâÔºâ" if long_text_mode else "Áü≠„ÅÑ1Êñá„ÅÆ„Åø„ÅßÊßãÊàê„Åô„ÇãÔºà„Éá„Éï„Ç©„É´„ÉàÔºâ"}

„ÄêÂá∫ÂäõÂΩ¢Âºè„Äë
‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
{{
    "japanese": "Êó•Êú¨Ë™û„ÅÆÊñáÁ´†",
    "english": "ÂØæÂøú„Åô„ÇãËã±Ë™û„ÅÆÊñáÁ´†",
    "difficulty": "easy/medium/hard",
    "category": "„Ç´„ÉÜ„Ç¥„É™Âêç"
}}

{"2-3Êñá„Åã„Çâ„Å™„Çã" if long_text_mode else "1„Å§„ÅÆ"}ÂïèÈ°å„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
"""

    return prompt
